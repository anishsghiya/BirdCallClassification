{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Imports","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Import some necessary libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport torch.nn.functional as F\nimport torchaudio\nfrom torchaudio import transforms\nfrom IPython.display import Audio\nimport torchvision\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-24T20:36:48.798642Z","iopub.execute_input":"2023-10-24T20:36:48.799108Z","iopub.status.idle":"2023-10-24T20:36:53.063014Z","shell.execute_reply.started":"2023-10-24T20:36:48.799066Z","shell.execute_reply":"2023-10-24T20:36:53.061186Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nimport pandas as pd\nimport os\n\nimport math, random","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:53.065917Z","iopub.execute_input":"2023-10-24T20:36:53.066838Z","iopub.status.idle":"2023-10-24T20:36:53.978031Z","shell.execute_reply.started":"2023-10-24T20:36:53.066777Z","shell.execute_reply":"2023-10-24T20:36:53.976264Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Set random seed","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed = 1999\nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:53.979931Z","iopub.execute_input":"2023-10-24T20:36:53.980400Z","iopub.status.idle":"2023-10-24T20:36:53.991389Z","shell.execute_reply.started":"2023-10-24T20:36:53.980349Z","shell.execute_reply":"2023-10-24T20:36:53.989963Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Some config parameters","metadata":{}},{"cell_type":"code","source":"class CFG:\n    isOneHot = False\n    rate = 32000\n    num_classes = 264","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:53.995177Z","iopub.execute_input":"2023-10-24T20:36:53.996399Z","iopub.status.idle":"2023-10-24T20:36:54.002504Z","shell.execute_reply.started":"2023-10-24T20:36:53.996343Z","shell.execute_reply":"2023-10-24T20:36:54.001500Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 1.4 Load a sample to get size and rate (32k Hz)","metadata":{}},{"cell_type":"code","source":"sig, sr = torchaudio.load('/kaggle/input/birdclef-2023/train_audio/abethr1/XC128013.ogg')\nprint('shape:',sig.shape)\nprint('rate:', sr)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:54.004278Z","iopub.execute_input":"2023-10-24T20:36:54.005116Z","iopub.status.idle":"2023-10-24T20:36:54.147166Z","shell.execute_reply.started":"2023-10-24T20:36:54.005063Z","shell.execute_reply":"2023-10-24T20:36:54.145778Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"shape: torch.Size([1, 1459513])\nrate: 32000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. MelSpectrogram for Vector Embedding","metadata":{}},{"cell_type":"code","source":"class AudioUtil():\n  @staticmethod\n  def open(audio_file):\n    sig, sr = torchaudio.load(audio_file)\n    return (sig, sr)\n\n  @staticmethod\n  def rechannel(aud, new_channel):\n    sig, sr = aud\n\n    if (sig.shape[0] == new_channel):\n      # Nothing to do\n      return aud\n\n    if (new_channel == 1):\n      # Convert from stereo to mono by selecting only the first channel\n      resig = sig[:1, :]\n    else:\n      # Convert from mono to stereo by duplicating the first channel\n      resig = torch.cat([sig, sig, sig])\n\n    return ((resig, sr))\n\n  @staticmethod\n  def resample(aud, newsr):\n    sig, sr = aud\n\n    if (sr == newsr):\n      # Nothing to do\n      return aud\n\n    num_channels = sig.shape[0]\n    # Resample first channel\n    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n    if (num_channels > 1):\n      # Resample the second channel and merge both channels\n      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n      resig = torch.cat([resig, retwo])\n\n    return ((resig, newsr))\n\n  @staticmethod\n  def pad_trunc(aud, max_ms):\n    sig, sr = aud\n    num_rows, sig_len = sig.shape\n    max_len = sr//1000 * max_ms\n\n    if (sig_len > max_len):\n      # Truncate the signal to the given length\n      sig = sig[:,:max_len]\n\n    elif (sig_len < max_len):\n      # Length of padding to add at the beginning and end of the signal\n      pad_begin_len = random.randint(0, max_len - sig_len)\n      pad_end_len = max_len - sig_len - pad_begin_len\n\n      # Pad with 0s\n      pad_begin = torch.zeros((num_rows, pad_begin_len))\n      pad_end = torch.zeros((num_rows, pad_end_len))\n\n      sig = torch.cat((pad_begin, sig, pad_end), 1)\n      \n    return (sig, sr)\n\n  @staticmethod\n  def time_shift(aud, shift_limit):\n    sig,sr = aud\n    _, sig_len = sig.shape\n    shift_amt = int(random.random() * shift_limit * sig_len)\n    return (sig.roll(shift_amt), sr)\n\n  @staticmethod\n  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n    sig,sr = aud\n    top_db = 80\n\n    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n    spec = torchaudio.transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n\n    # Convert to decibels\n    spec = torchaudio.transforms.AmplitudeToDB(top_db=top_db)(spec)\n    return (spec)\n\n  @staticmethod\n  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n    _, n_mels, n_steps = spec.shape\n    mask_value = spec.mean()\n    aug_spec = spec\n\n    freq_mask_param = max_mask_pct * n_mels\n    for _ in range(n_freq_masks):\n      aug_spec = torchaudio.transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n\n    time_mask_param = max_mask_pct * n_steps\n    for _ in range(n_time_masks):\n      aug_spec = torchaudio.transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n\n    return aug_spec","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:54.149906Z","iopub.execute_input":"2023-10-24T20:36:54.150821Z","iopub.status.idle":"2023-10-24T20:36:54.176843Z","shell.execute_reply.started":"2023-10-24T20:36:54.150762Z","shell.execute_reply":"2023-10-24T20:36:54.175575Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 3. Lable Encoder","metadata":{}},{"cell_type":"code","source":"meta_df = pd.read_csv('/kaggle/input/birdclef-2023/train_metadata.csv')\nprint('data shape:',meta_df.shape)\nmeta_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:54.178346Z","iopub.execute_input":"2023-10-24T20:36:54.179689Z","iopub.status.idle":"2023-10-24T20:36:54.387843Z","shell.execute_reply.started":"2023-10-24T20:36:54.179643Z","shell.execute_reply":"2023-10-24T20:36:54.386141Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"data shape: (16941, 12)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  primary_label secondary_labels              type  latitude  longitude  \\\n0       abethr1               []          ['song']    4.3906    38.2788   \n1       abethr1               []          ['call']   -2.9524    38.2921   \n2       abethr1               []          ['song']   -2.9524    38.2921   \n3       abethr1               []          ['song']   -2.9524    38.2921   \n4       abethr1               []  ['call', 'song']   -2.9524    38.2921   \n\n      scientific_name               common_name         author  \\\n0  Turdus tephronotus  African Bare-eyed Thrush  Rolf A. de By   \n1  Turdus tephronotus  African Bare-eyed Thrush  James Bradley   \n2  Turdus tephronotus  African Bare-eyed Thrush  James Bradley   \n3  Turdus tephronotus  African Bare-eyed Thrush  James Bradley   \n4  Turdus tephronotus  African Bare-eyed Thrush  James Bradley   \n\n                                             license  rating  \\\n0  Creative Commons Attribution-NonCommercial-Sha...     4.0   \n1  Creative Commons Attribution-NonCommercial-Sha...     3.5   \n2  Creative Commons Attribution-NonCommercial-Sha...     3.5   \n3  Creative Commons Attribution-NonCommercial-Sha...     5.0   \n4  Creative Commons Attribution-NonCommercial-Sha...     4.5   \n\n                                 url              filename  \n0  https://www.xeno-canto.org/128013  abethr1/XC128013.ogg  \n1  https://www.xeno-canto.org/363501  abethr1/XC363501.ogg  \n2  https://www.xeno-canto.org/363502  abethr1/XC363502.ogg  \n3  https://www.xeno-canto.org/363503  abethr1/XC363503.ogg  \n4  https://www.xeno-canto.org/363504  abethr1/XC363504.ogg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>primary_label</th>\n      <th>secondary_labels</th>\n      <th>type</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>scientific_name</th>\n      <th>common_name</th>\n      <th>author</th>\n      <th>license</th>\n      <th>rating</th>\n      <th>url</th>\n      <th>filename</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abethr1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>4.3906</td>\n      <td>38.2788</td>\n      <td>Turdus tephronotus</td>\n      <td>African Bare-eyed Thrush</td>\n      <td>Rolf A. de By</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>4.0</td>\n      <td>https://www.xeno-canto.org/128013</td>\n      <td>abethr1/XC128013.ogg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>abethr1</td>\n      <td>[]</td>\n      <td>['call']</td>\n      <td>-2.9524</td>\n      <td>38.2921</td>\n      <td>Turdus tephronotus</td>\n      <td>African Bare-eyed Thrush</td>\n      <td>James Bradley</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>3.5</td>\n      <td>https://www.xeno-canto.org/363501</td>\n      <td>abethr1/XC363501.ogg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>abethr1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>-2.9524</td>\n      <td>38.2921</td>\n      <td>Turdus tephronotus</td>\n      <td>African Bare-eyed Thrush</td>\n      <td>James Bradley</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>3.5</td>\n      <td>https://www.xeno-canto.org/363502</td>\n      <td>abethr1/XC363502.ogg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>abethr1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>-2.9524</td>\n      <td>38.2921</td>\n      <td>Turdus tephronotus</td>\n      <td>African Bare-eyed Thrush</td>\n      <td>James Bradley</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>5.0</td>\n      <td>https://www.xeno-canto.org/363503</td>\n      <td>abethr1/XC363503.ogg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>abethr1</td>\n      <td>[]</td>\n      <td>['call', 'song']</td>\n      <td>-2.9524</td>\n      <td>38.2921</td>\n      <td>Turdus tephronotus</td>\n      <td>African Bare-eyed Thrush</td>\n      <td>James Bradley</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>4.5</td>\n      <td>https://www.xeno-canto.org/363504</td>\n      <td>abethr1/XC363504.ogg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('Number of classes and number samples in each class:',meta_df.primary_label.value_counts().reset_index().shape)\nprint('Number of classes with more than 1 sample:',meta_df.primary_label.value_counts().reset_index()\\\n                                                              .query('primary_label > 1').shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:54.389490Z","iopub.execute_input":"2023-10-24T20:36:54.389890Z","iopub.status.idle":"2023-10-24T20:36:54.424718Z","shell.execute_reply.started":"2023-10-24T20:36:54.389850Z","shell.execute_reply":"2023-10-24T20:36:54.423199Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Number of classes and number samples in each class: (264, 2)\nNumber of classes with more than 1 sample: (257, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"if CFG.isOneHot:\n    label = OneHotEncoder(sparse=False)\\\n                                    .fit_transform(meta_df['primary_label'].to_numpy().reshape(-1,1))\n    meta_df['label'] = pd.DataFrame(label).apply(lambda x: list(x), axis = 1)\nelse:\n    meta_df['label'] = LabelEncoder().fit_transform(meta_df['primary_label'])\nmeta_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:54.426357Z","iopub.execute_input":"2023-10-24T20:36:54.426858Z","iopub.status.idle":"2023-10-24T20:36:54.462329Z","shell.execute_reply.started":"2023-10-24T20:36:54.426817Z","shell.execute_reply":"2023-10-24T20:36:54.460093Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  primary_label secondary_labels      type  latitude  longitude  \\\n0       abethr1               []  ['song']    4.3906    38.2788   \n1       abethr1               []  ['call']   -2.9524    38.2921   \n\n      scientific_name               common_name         author  \\\n0  Turdus tephronotus  African Bare-eyed Thrush  Rolf A. de By   \n1  Turdus tephronotus  African Bare-eyed Thrush  James Bradley   \n\n                                             license  rating  \\\n0  Creative Commons Attribution-NonCommercial-Sha...     4.0   \n1  Creative Commons Attribution-NonCommercial-Sha...     3.5   \n\n                                 url              filename  label  \n0  https://www.xeno-canto.org/128013  abethr1/XC128013.ogg      0  \n1  https://www.xeno-canto.org/363501  abethr1/XC363501.ogg      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>primary_label</th>\n      <th>secondary_labels</th>\n      <th>type</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>scientific_name</th>\n      <th>common_name</th>\n      <th>author</th>\n      <th>license</th>\n      <th>rating</th>\n      <th>url</th>\n      <th>filename</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abethr1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>4.3906</td>\n      <td>38.2788</td>\n      <td>Turdus tephronotus</td>\n      <td>African Bare-eyed Thrush</td>\n      <td>Rolf A. de By</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>4.0</td>\n      <td>https://www.xeno-canto.org/128013</td>\n      <td>abethr1/XC128013.ogg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>abethr1</td>\n      <td>[]</td>\n      <td>['call']</td>\n      <td>-2.9524</td>\n      <td>38.2921</td>\n      <td>Turdus tephronotus</td>\n      <td>African Bare-eyed Thrush</td>\n      <td>James Bradley</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>3.5</td>\n      <td>https://www.xeno-canto.org/363501</td>\n      <td>abethr1/XC363501.ogg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 4. Define DataLoader and Fold Training/Validation","metadata":{}},{"cell_type":"markdown","source":"## 4.1 5-Fold Split","metadata":{}},{"cell_type":"code","source":"def cv_split(Xtrain, ytrain, n_folds, seed):\n    kfold = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed)\n    for num, (train_index, val_index) in enumerate(kfold.split(Xtrain, ytrain)):\n        Xtrain.loc[val_index, 'fold'] = int(num)\n    Xtrain['fold'] = Xtrain['fold'].astype(int)\n    return Xtrain\n\nmeta_df = cv_split(meta_df,meta_df['primary_label'], 5, 42)\nmeta_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:54.467838Z","iopub.execute_input":"2023-10-24T20:36:54.469156Z","iopub.status.idle":"2023-10-24T20:36:54.539799Z","shell.execute_reply.started":"2023-10-24T20:36:54.469112Z","shell.execute_reply":"2023-10-24T20:36:54.538053Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n  UserWarning,\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"  primary_label secondary_labels      type  latitude  longitude  \\\n0       abethr1               []  ['song']    4.3906    38.2788   \n1       abethr1               []  ['call']   -2.9524    38.2921   \n\n      scientific_name               common_name         author  \\\n0  Turdus tephronotus  African Bare-eyed Thrush  Rolf A. de By   \n1  Turdus tephronotus  African Bare-eyed Thrush  James Bradley   \n\n                                             license  rating  \\\n0  Creative Commons Attribution-NonCommercial-Sha...     4.0   \n1  Creative Commons Attribution-NonCommercial-Sha...     3.5   \n\n                                 url              filename  label  fold  \n0  https://www.xeno-canto.org/128013  abethr1/XC128013.ogg      0     3  \n1  https://www.xeno-canto.org/363501  abethr1/XC363501.ogg      0     3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>primary_label</th>\n      <th>secondary_labels</th>\n      <th>type</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>scientific_name</th>\n      <th>common_name</th>\n      <th>author</th>\n      <th>license</th>\n      <th>rating</th>\n      <th>url</th>\n      <th>filename</th>\n      <th>label</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abethr1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>4.3906</td>\n      <td>38.2788</td>\n      <td>Turdus tephronotus</td>\n      <td>African Bare-eyed Thrush</td>\n      <td>Rolf A. de By</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>4.0</td>\n      <td>https://www.xeno-canto.org/128013</td>\n      <td>abethr1/XC128013.ogg</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>abethr1</td>\n      <td>[]</td>\n      <td>['call']</td>\n      <td>-2.9524</td>\n      <td>38.2921</td>\n      <td>Turdus tephronotus</td>\n      <td>African Bare-eyed Thrush</td>\n      <td>James Bradley</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>3.5</td>\n      <td>https://www.xeno-canto.org/363501</td>\n      <td>abethr1/XC363501.ogg</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 4.2 DataLoader ","metadata":{}},{"cell_type":"code","source":"# ----------------------------\n# Sound Dataset\n# ----------------------------\nclass SoundDS(Dataset):\n  def __init__(self, df, data_path ='/kaggle/input/birdclef-2023/train_audio/'):\n    self.df = df\n    self.data_path = str(data_path)\n    self.duration = 8000\n    self.sr = 32000\n    self.channel = 3\n    self.shift_pct = 0.4\n    \n            \n  # ----------------------------\n  # Number of items in dataset\n  # ----------------------------\n  def __len__(self):\n    return len(self.df)    \n    \n  def __getitem__(self, idx):\n\n    audio_file = self.data_path + self.df.loc[idx, 'filename']\n    # Get the Class ID\n    class_id = np.array(self.df.loc[idx, 'label'])\n    aud = AudioUtil.open(audio_file)\n    reaud = AudioUtil.resample(aud, self.sr)\n    rechan = AudioUtil.rechannel(reaud, self.channel)\n    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n    return aug_sgram, class_id","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:54.541791Z","iopub.execute_input":"2023-10-24T20:36:54.542847Z","iopub.status.idle":"2023-10-24T20:36:54.558342Z","shell.execute_reply.started":"2023-10-24T20:36:54.542795Z","shell.execute_reply":"2023-10-24T20:36:54.556765Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_ds = SoundDS(meta_df[meta_df.fold != 0].reset_index())\nval_ds = SoundDS(meta_df[meta_df.fold == 0].reset_index())\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:54.560742Z","iopub.execute_input":"2023-10-24T20:36:54.561220Z","iopub.status.idle":"2023-10-24T20:36:54.584817Z","shell.execute_reply.started":"2023-10-24T20:36:54.561169Z","shell.execute_reply":"2023-10-24T20:36:54.583555Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 Test DataLoader","metadata":{}},{"cell_type":"code","source":"a,b = next(iter(train_dl))\nprint(a.shape)\nprint(b.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:36:54.589356Z","iopub.execute_input":"2023-10-24T20:36:54.589795Z","iopub.status.idle":"2023-10-24T20:37:05.834445Z","shell.execute_reply.started":"2023-10-24T20:36:54.589754Z","shell.execute_reply":"2023-10-24T20:37:05.832700Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"torch.Size([128, 3, 64, 501])\ntorch.Size([128])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5. Import Backbone Pretrained Model","metadata":{}},{"cell_type":"code","source":"model = torchvision.models.resnet34(pretrained=True)\n\n\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, CFG.num_classes)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = model.to(device)\nnext(myModel.parameters()).device","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:37:05.838928Z","iopub.execute_input":"2023-10-24T20:37:05.839355Z","iopub.status.idle":"2023-10-24T20:37:07.222572Z","shell.execute_reply.started":"2023-10-24T20:37:05.839316Z","shell.execute_reply":"2023-10-24T20:37:07.221114Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/83.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7c2e61c0f5746e3ade7f71888c65f9d"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"markdown","source":"# 6. Start Training ","metadata":{}},{"cell_type":"code","source":"def training(model, train_dl, val_dl, num_epochs):\n  # Loss Function, Optimizer and Scheduler\n  criterion = nn.CrossEntropyLoss()\n  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\n                                                steps_per_epoch=int(len(train_dl)),\n                                                epochs=num_epochs,\n                                                anneal_strategy='linear')\n\n  # Repeat for each epoch\n  best_acc = -1\n  for epoch in range(num_epochs):\n    running_loss = 0.0\n    correct_prediction = 0\n    total_prediction = 0\n\n    # Repeat for each batch in the training set\n    for i, data in enumerate(train_dl):\n        # Get the input features and target labels, and put them on the GPU\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # Normalize the inputs\n        inputs_m, inputs_s = inputs.mean(), inputs.std()\n        inputs = (inputs - inputs_m) / inputs_s\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # Keep stats for Loss and Accuracy\n        running_loss += loss.item()\n\n        # Get the predicted class with the highest score\n        _, prediction = torch.max(outputs,1)\n        # Count of predictions that matched the target label\n        correct_prediction += (prediction == labels).sum().item()\n        total_prediction += prediction.shape[0]\n\n        if (i + 1) % 1 == 0:    # print every 10 mini-batches\n           print('Epoch [{}/{}], Step [{}/{}], Loss : {:.4f}'\n            .format(epoch + 1, num_epochs, i + 1, len(train_dl), running_loss/(i + 1)))\n    \n    # Print stats at the end of the epoch\n    num_batches = len(train_dl)\n    avg_loss = running_loss / num_batches\n    acc = correct_prediction/total_prediction\n    print(f'Epoch: {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}')\n\n    gt = []\n    pred = []\n\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        val_loss = 0 \n        for idx, data_ in enumerate(val_dl):\n            inputs, labels = data_[0].to(device), data_[1].to(device)\n\n            # Normalize the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            # forward + backward + optimize\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Keep stats for Loss and Accuracy\n            val_loss += loss.item()\n\n            # Get the predicted class with the highest score\n            _, prediction = torch.max(outputs,1)\n            predi = torch.softmax(outputs, dim = -1)\n            gt.append(labels)\n            pred.append(predi[:,1])\n            # Count of predictions that matched the target label\n            correct += (prediction == labels).sum().item()\n            total += prediction.shape[0]\n        print('Accuracy of the network val voices: {:.4f} %'.format(100 * correct / total))\n\n        final_score = 100 * correct / total\n\n\n        if (best_acc < final_score):\n            best_acc = final_score\n            print(\"Saving best model!\")\n            torch.save(model.state_dict(), f'BirdSound_MobileNetV2_fold0_epoch{epoch}.pth')\n\n  print('Finished Training')\n  \nnum_epochs = 15   # Just for demo, adjust this higher.\ntraining(myModel, train_dl, val_dl, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T20:37:07.224646Z","iopub.execute_input":"2023-10-24T20:37:07.225507Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch [1/15], Step [1/106], Loss : 5.9149\nEpoch [1/15], Step [2/106], Loss : 5.7877\nEpoch [1/15], Step [3/106], Loss : 5.7286\nEpoch [1/15], Step [4/106], Loss : 5.6391\nEpoch [1/15], Step [5/106], Loss : 5.5584\nEpoch [1/15], Step [6/106], Loss : 5.4752\nEpoch [1/15], Step [7/106], Loss : 5.4383\nEpoch [1/15], Step [8/106], Loss : 5.3703\nEpoch [1/15], Step [9/106], Loss : 5.3238\nEpoch [1/15], Step [10/106], Loss : 5.2602\nEpoch [1/15], Step [11/106], Loss : 5.2123\nEpoch [1/15], Step [12/106], Loss : 5.1682\nEpoch [1/15], Step [13/106], Loss : 5.1269\nEpoch [1/15], Step [14/106], Loss : 5.1091\nEpoch [1/15], Step [15/106], Loss : 5.0717\nEpoch [1/15], Step [16/106], Loss : 5.0291\nEpoch [1/15], Step [17/106], Loss : 5.0090\nEpoch [1/15], Step [18/106], Loss : 4.9728\nEpoch [1/15], Step [19/106], Loss : 4.9722\nEpoch [1/15], Step [20/106], Loss : 4.9426\nEpoch [1/15], Step [21/106], Loss : 4.9250\nEpoch [1/15], Step [22/106], Loss : 4.8826\nEpoch [1/15], Step [23/106], Loss : 4.8511\nEpoch [1/15], Step [24/106], Loss : 4.8196\nEpoch [1/15], Step [25/106], Loss : 4.8000\nEpoch [1/15], Step [26/106], Loss : 4.7783\nEpoch [1/15], Step [27/106], Loss : 4.7633\nEpoch [1/15], Step [28/106], Loss : 4.7308\nEpoch [1/15], Step [29/106], Loss : 4.7114\nEpoch [1/15], Step [30/106], Loss : 4.6853\nEpoch [1/15], Step [31/106], Loss : 4.6618\nEpoch [1/15], Step [32/106], Loss : 4.6436\nEpoch [1/15], Step [33/106], Loss : 4.6202\nEpoch [1/15], Step [34/106], Loss : 4.6035\nEpoch [1/15], Step [35/106], Loss : 4.5924\nEpoch [1/15], Step [36/106], Loss : 4.5797\nEpoch [1/15], Step [37/106], Loss : 4.5677\nEpoch [1/15], Step [38/106], Loss : 4.5480\nEpoch [1/15], Step [39/106], Loss : 4.5383\nEpoch [1/15], Step [40/106], Loss : 4.5149\nEpoch [1/15], Step [41/106], Loss : 4.4985\nEpoch [1/15], Step [42/106], Loss : 4.4830\nEpoch [1/15], Step [43/106], Loss : 4.4659\nEpoch [1/15], Step [44/106], Loss : 4.4579\nEpoch [1/15], Step [45/106], Loss : 4.4470\nEpoch [1/15], Step [46/106], Loss : 4.4316\nEpoch [1/15], Step [47/106], Loss : 4.4166\nEpoch [1/15], Step [48/106], Loss : 4.4043\nEpoch [1/15], Step [49/106], Loss : 4.3916\nEpoch [1/15], Step [50/106], Loss : 4.3773\nEpoch [1/15], Step [51/106], Loss : 4.3611\nEpoch [1/15], Step [52/106], Loss : 4.3436\nEpoch [1/15], Step [53/106], Loss : 4.3337\nEpoch [1/15], Step [54/106], Loss : 4.3246\nEpoch [1/15], Step [55/106], Loss : 4.3076\nEpoch [1/15], Step [56/106], Loss : 4.2926\nEpoch [1/15], Step [57/106], Loss : 4.2839\nEpoch [1/15], Step [58/106], Loss : 4.2698\nEpoch [1/15], Step [59/106], Loss : 4.2576\nEpoch [1/15], Step [60/106], Loss : 4.2463\nEpoch [1/15], Step [61/106], Loss : 4.2384\nEpoch [1/15], Step [62/106], Loss : 4.2226\nEpoch [1/15], Step [63/106], Loss : 4.2062\nEpoch [1/15], Step [64/106], Loss : 4.2009\nEpoch [1/15], Step [65/106], Loss : 4.1938\nEpoch [1/15], Step [66/106], Loss : 4.1888\nEpoch [1/15], Step [67/106], Loss : 4.1756\nEpoch [1/15], Step [68/106], Loss : 4.1672\nEpoch [1/15], Step [69/106], Loss : 4.1545\nEpoch [1/15], Step [70/106], Loss : 4.1447\nEpoch [1/15], Step [71/106], Loss : 4.1319\nEpoch [1/15], Step [72/106], Loss : 4.1221\nEpoch [1/15], Step [73/106], Loss : 4.1078\nEpoch [1/15], Step [74/106], Loss : 4.1015\nEpoch [1/15], Step [75/106], Loss : 4.0941\nEpoch [1/15], Step [76/106], Loss : 4.0821\nEpoch [1/15], Step [77/106], Loss : 4.0758\nEpoch [1/15], Step [78/106], Loss : 4.0645\nEpoch [1/15], Step [79/106], Loss : 4.0571\nEpoch [1/15], Step [80/106], Loss : 4.0488\nEpoch [1/15], Step [81/106], Loss : 4.0450\nEpoch [1/15], Step [82/106], Loss : 4.0381\nEpoch [1/15], Step [83/106], Loss : 4.0291\nEpoch [1/15], Step [84/106], Loss : 4.0173\nEpoch [1/15], Step [85/106], Loss : 4.0133\nEpoch [1/15], Step [86/106], Loss : 4.0005\nEpoch [1/15], Step [87/106], Loss : 3.9969\nEpoch [1/15], Step [88/106], Loss : 3.9922\nEpoch [1/15], Step [89/106], Loss : 3.9874\nEpoch [1/15], Step [90/106], Loss : 3.9796\nEpoch [1/15], Step [91/106], Loss : 3.9736\nEpoch [1/15], Step [92/106], Loss : 3.9663\nEpoch [1/15], Step [93/106], Loss : 3.9628\nEpoch [1/15], Step [94/106], Loss : 3.9558\nEpoch [1/15], Step [95/106], Loss : 3.9496\nEpoch [1/15], Step [96/106], Loss : 3.9405\nEpoch [1/15], Step [97/106], Loss : 3.9344\nEpoch [1/15], Step [98/106], Loss : 3.9229\nEpoch [1/15], Step [99/106], Loss : 3.9169\nEpoch [1/15], Step [100/106], Loss : 3.9109\nEpoch [1/15], Step [101/106], Loss : 3.9064\nEpoch [1/15], Step [102/106], Loss : 3.9045\nEpoch [1/15], Step [103/106], Loss : 3.8997\nEpoch [1/15], Step [104/106], Loss : 3.8926\nEpoch [1/15], Step [105/106], Loss : 3.8867\nEpoch [1/15], Step [106/106], Loss : 3.8803\nEpoch: 1, Loss: 3.8803, Accuracy: 0.2368\nAccuracy of the network val voices: 31.8383 %\nSaving best model!\nEpoch [2/15], Step [1/106], Loss : 3.0735\nEpoch [2/15], Step [2/106], Loss : 3.0930\nEpoch [2/15], Step [3/106], Loss : 3.1304\nEpoch [2/15], Step [4/106], Loss : 3.2107\nEpoch [2/15], Step [5/106], Loss : 3.2116\nEpoch [2/15], Step [6/106], Loss : 3.2134\nEpoch [2/15], Step [7/106], Loss : 3.2376\nEpoch [2/15], Step [8/106], Loss : 3.2113\nEpoch [2/15], Step [9/106], Loss : 3.1558\nEpoch [2/15], Step [10/106], Loss : 3.1476\nEpoch [2/15], Step [11/106], Loss : 3.1116\nEpoch [2/15], Step [12/106], Loss : 3.1244\nEpoch [2/15], Step [13/106], Loss : 3.1218\nEpoch [2/15], Step [14/106], Loss : 3.1515\nEpoch [2/15], Step [15/106], Loss : 3.1459\nEpoch [2/15], Step [16/106], Loss : 3.1275\nEpoch [2/15], Step [17/106], Loss : 3.1242\nEpoch [2/15], Step [18/106], Loss : 3.1181\nEpoch [2/15], Step [19/106], Loss : 3.1073\nEpoch [2/15], Step [20/106], Loss : 3.1113\nEpoch [2/15], Step [21/106], Loss : 3.0941\nEpoch [2/15], Step [22/106], Loss : 3.1110\nEpoch [2/15], Step [23/106], Loss : 3.1075\nEpoch [2/15], Step [24/106], Loss : 3.1125\nEpoch [2/15], Step [25/106], Loss : 3.1267\nEpoch [2/15], Step [26/106], Loss : 3.1205\nEpoch [2/15], Step [27/106], Loss : 3.1115\nEpoch [2/15], Step [28/106], Loss : 3.1061\nEpoch [2/15], Step [29/106], Loss : 3.1048\nEpoch [2/15], Step [30/106], Loss : 3.0926\nEpoch [2/15], Step [31/106], Loss : 3.0961\nEpoch [2/15], Step [32/106], Loss : 3.0892\nEpoch [2/15], Step [33/106], Loss : 3.0896\nEpoch [2/15], Step [34/106], Loss : 3.0963\nEpoch [2/15], Step [35/106], Loss : 3.0971\nEpoch [2/15], Step [36/106], Loss : 3.0955\nEpoch [2/15], Step [37/106], Loss : 3.1042\nEpoch [2/15], Step [38/106], Loss : 3.1114\nEpoch [2/15], Step [39/106], Loss : 3.1059\nEpoch [2/15], Step [40/106], Loss : 3.1082\nEpoch [2/15], Step [41/106], Loss : 3.1100\nEpoch [2/15], Step [42/106], Loss : 3.1071\nEpoch [2/15], Step [43/106], Loss : 3.1081\nEpoch [2/15], Step [44/106], Loss : 3.1086\nEpoch [2/15], Step [45/106], Loss : 3.1152\nEpoch [2/15], Step [46/106], Loss : 3.1152\nEpoch [2/15], Step [47/106], Loss : 3.1209\nEpoch [2/15], Step [48/106], Loss : 3.1157\nEpoch [2/15], Step [49/106], Loss : 3.1161\nEpoch [2/15], Step [50/106], Loss : 3.1149\nEpoch [2/15], Step [51/106], Loss : 3.1221\nEpoch [2/15], Step [52/106], Loss : 3.1185\nEpoch [2/15], Step [53/106], Loss : 3.1133\nEpoch [2/15], Step [54/106], Loss : 3.1120\nEpoch [2/15], Step [55/106], Loss : 3.1092\nEpoch [2/15], Step [56/106], Loss : 3.1063\nEpoch [2/15], Step [57/106], Loss : 3.1040\nEpoch [2/15], Step [58/106], Loss : 3.0958\nEpoch [2/15], Step [59/106], Loss : 3.0933\nEpoch [2/15], Step [60/106], Loss : 3.0924\nEpoch [2/15], Step [61/106], Loss : 3.0848\nEpoch [2/15], Step [62/106], Loss : 3.0844\nEpoch [2/15], Step [63/106], Loss : 3.0818\nEpoch [2/15], Step [64/106], Loss : 3.0785\nEpoch [2/15], Step [65/106], Loss : 3.0802\nEpoch [2/15], Step [66/106], Loss : 3.0767\nEpoch [2/15], Step [67/106], Loss : 3.0838\nEpoch [2/15], Step [68/106], Loss : 3.0797\nEpoch [2/15], Step [69/106], Loss : 3.0817\nEpoch [2/15], Step [70/106], Loss : 3.0816\nEpoch [2/15], Step [71/106], Loss : 3.0817\nEpoch [2/15], Step [72/106], Loss : 3.0758\nEpoch [2/15], Step [73/106], Loss : 3.0777\nEpoch [2/15], Step [74/106], Loss : 3.0819\nEpoch [2/15], Step [75/106], Loss : 3.0816\nEpoch [2/15], Step [76/106], Loss : 3.0755\nEpoch [2/15], Step [77/106], Loss : 3.0754\nEpoch [2/15], Step [78/106], Loss : 3.0743\nEpoch [2/15], Step [79/106], Loss : 3.0742\nEpoch [2/15], Step [80/106], Loss : 3.0705\nEpoch [2/15], Step [81/106], Loss : 3.0696\nEpoch [2/15], Step [82/106], Loss : 3.0678\nEpoch [2/15], Step [83/106], Loss : 3.0706\nEpoch [2/15], Step [84/106], Loss : 3.0682\nEpoch [2/15], Step [85/106], Loss : 3.0643\nEpoch [2/15], Step [86/106], Loss : 3.0676\nEpoch [2/15], Step [87/106], Loss : 3.0690\nEpoch [2/15], Step [88/106], Loss : 3.0712\nEpoch [2/15], Step [89/106], Loss : 3.0717\nEpoch [2/15], Step [90/106], Loss : 3.0722\nEpoch [2/15], Step [91/106], Loss : 3.0710\nEpoch [2/15], Step [92/106], Loss : 3.0719\nEpoch [2/15], Step [93/106], Loss : 3.0697\nEpoch [2/15], Step [94/106], Loss : 3.0665\nEpoch [2/15], Step [95/106], Loss : 3.0691\nEpoch [2/15], Step [96/106], Loss : 3.0657\nEpoch [2/15], Step [97/106], Loss : 3.0635\nEpoch [2/15], Step [98/106], Loss : 3.0609\nEpoch [2/15], Step [99/106], Loss : 3.0587\nEpoch [2/15], Step [100/106], Loss : 3.0554\nEpoch [2/15], Step [101/106], Loss : 3.0553\nEpoch [2/15], Step [102/106], Loss : 3.0514\nEpoch [2/15], Step [103/106], Loss : 3.0502\nEpoch [2/15], Step [104/106], Loss : 3.0495\nEpoch [2/15], Step [105/106], Loss : 3.0501\nEpoch [2/15], Step [106/106], Loss : 3.0475\nEpoch: 2, Loss: 3.0475, Accuracy: 0.3487\nAccuracy of the network val voices: 36.9726 %\nSaving best model!\nEpoch [3/15], Step [1/106], Loss : 2.5766\nEpoch [3/15], Step [2/106], Loss : 2.7201\nEpoch [3/15], Step [3/106], Loss : 2.7468\nEpoch [3/15], Step [4/106], Loss : 2.7864\nEpoch [3/15], Step [5/106], Loss : 2.7875\nEpoch [3/15], Step [6/106], Loss : 2.8689\nEpoch [3/15], Step [7/106], Loss : 2.8851\nEpoch [3/15], Step [8/106], Loss : 2.8863\nEpoch [3/15], Step [9/106], Loss : 2.8736\nEpoch [3/15], Step [10/106], Loss : 2.8607\nEpoch [3/15], Step [11/106], Loss : 2.8789\nEpoch [3/15], Step [12/106], Loss : 2.8823\nEpoch [3/15], Step [13/106], Loss : 2.8544\nEpoch [3/15], Step [14/106], Loss : 2.8496\nEpoch [3/15], Step [15/106], Loss : 2.8466\nEpoch [3/15], Step [16/106], Loss : 2.8518\nEpoch [3/15], Step [17/106], Loss : 2.8407\nEpoch [3/15], Step [18/106], Loss : 2.8468\nEpoch [3/15], Step [19/106], Loss : 2.8508\nEpoch [3/15], Step [20/106], Loss : 2.8582\nEpoch [3/15], Step [21/106], Loss : 2.8513\nEpoch [3/15], Step [22/106], Loss : 2.8339\nEpoch [3/15], Step [23/106], Loss : 2.8299\nEpoch [3/15], Step [24/106], Loss : 2.8314\nEpoch [3/15], Step [25/106], Loss : 2.8337\nEpoch [3/15], Step [26/106], Loss : 2.8482\nEpoch [3/15], Step [27/106], Loss : 2.8483\nEpoch [3/15], Step [28/106], Loss : 2.8503\nEpoch [3/15], Step [29/106], Loss : 2.8690\nEpoch [3/15], Step [30/106], Loss : 2.8761\nEpoch [3/15], Step [31/106], Loss : 2.8735\nEpoch [3/15], Step [32/106], Loss : 2.8827\nEpoch [3/15], Step [33/106], Loss : 2.8942\nEpoch [3/15], Step [34/106], Loss : 2.8874\nEpoch [3/15], Step [35/106], Loss : 2.8871\nEpoch [3/15], Step [36/106], Loss : 2.8797\nEpoch [3/15], Step [37/106], Loss : 2.8690\nEpoch [3/15], Step [38/106], Loss : 2.8590\nEpoch [3/15], Step [39/106], Loss : 2.8623\nEpoch [3/15], Step [40/106], Loss : 2.8630\nEpoch [3/15], Step [41/106], Loss : 2.8628\nEpoch [3/15], Step [42/106], Loss : 2.8551\nEpoch [3/15], Step [43/106], Loss : 2.8452\nEpoch [3/15], Step [44/106], Loss : 2.8390\nEpoch [3/15], Step [45/106], Loss : 2.8363\nEpoch [3/15], Step [46/106], Loss : 2.8284\nEpoch [3/15], Step [47/106], Loss : 2.8301\nEpoch [3/15], Step [48/106], Loss : 2.8280\nEpoch [3/15], Step [49/106], Loss : 2.8219\nEpoch [3/15], Step [50/106], Loss : 2.8181\nEpoch [3/15], Step [51/106], Loss : 2.8054\nEpoch [3/15], Step [52/106], Loss : 2.8052\nEpoch [3/15], Step [53/106], Loss : 2.8031\nEpoch [3/15], Step [54/106], Loss : 2.7966\nEpoch [3/15], Step [55/106], Loss : 2.7979\nEpoch [3/15], Step [56/106], Loss : 2.7944\nEpoch [3/15], Step [57/106], Loss : 2.7923\nEpoch [3/15], Step [58/106], Loss : 2.8030\nEpoch [3/15], Step [59/106], Loss : 2.8002\nEpoch [3/15], Step [60/106], Loss : 2.8013\nEpoch [3/15], Step [61/106], Loss : 2.8030\nEpoch [3/15], Step [62/106], Loss : 2.8050\nEpoch [3/15], Step [63/106], Loss : 2.8018\nEpoch [3/15], Step [64/106], Loss : 2.7981\nEpoch [3/15], Step [65/106], Loss : 2.7989\nEpoch [3/15], Step [66/106], Loss : 2.7962\nEpoch [3/15], Step [67/106], Loss : 2.7924\nEpoch [3/15], Step [68/106], Loss : 2.7882\nEpoch [3/15], Step [69/106], Loss : 2.7903\nEpoch [3/15], Step [70/106], Loss : 2.7848\nEpoch [3/15], Step [71/106], Loss : 2.7806\nEpoch [3/15], Step [72/106], Loss : 2.7762\nEpoch [3/15], Step [73/106], Loss : 2.7738\nEpoch [3/15], Step [74/106], Loss : 2.7730\nEpoch [3/15], Step [75/106], Loss : 2.7723\nEpoch [3/15], Step [76/106], Loss : 2.7714\nEpoch [3/15], Step [77/106], Loss : 2.7706\nEpoch [3/15], Step [78/106], Loss : 2.7677\nEpoch [3/15], Step [79/106], Loss : 2.7612\nEpoch [3/15], Step [80/106], Loss : 2.7595\nEpoch [3/15], Step [81/106], Loss : 2.7589\nEpoch [3/15], Step [82/106], Loss : 2.7581\nEpoch [3/15], Step [83/106], Loss : 2.7623\nEpoch [3/15], Step [84/106], Loss : 2.7612\nEpoch [3/15], Step [85/106], Loss : 2.7595\nEpoch [3/15], Step [86/106], Loss : 2.7648\nEpoch [3/15], Step [87/106], Loss : 2.7673\nEpoch [3/15], Step [88/106], Loss : 2.7654\nEpoch [3/15], Step [89/106], Loss : 2.7664\nEpoch [3/15], Step [90/106], Loss : 2.7653\nEpoch [3/15], Step [91/106], Loss : 2.7677\nEpoch [3/15], Step [92/106], Loss : 2.7688\nEpoch [3/15], Step [93/106], Loss : 2.7662\nEpoch [3/15], Step [94/106], Loss : 2.7704\nEpoch [3/15], Step [95/106], Loss : 2.7702\nEpoch [3/15], Step [96/106], Loss : 2.7661\nEpoch [3/15], Step [97/106], Loss : 2.7647\nEpoch [3/15], Step [98/106], Loss : 2.7656\nEpoch [3/15], Step [99/106], Loss : 2.7640\nEpoch [3/15], Step [100/106], Loss : 2.7652\nEpoch [3/15], Step [101/106], Loss : 2.7683\nEpoch [3/15], Step [102/106], Loss : 2.7683\nEpoch [3/15], Step [103/106], Loss : 2.7671\nEpoch [3/15], Step [104/106], Loss : 2.7682\nEpoch [3/15], Step [105/106], Loss : 2.7698\nEpoch [3/15], Step [106/106], Loss : 2.7662\nEpoch: 3, Loss: 2.7662, Accuracy: 0.3975\nAccuracy of the network val voices: 39.7462 %\nSaving best model!\nEpoch [4/15], Step [1/106], Loss : 2.4862\nEpoch [4/15], Step [2/106], Loss : 2.7307\nEpoch [4/15], Step [3/106], Loss : 2.7416\nEpoch [4/15], Step [4/106], Loss : 2.6903\nEpoch [4/15], Step [5/106], Loss : 2.6629\nEpoch [4/15], Step [6/106], Loss : 2.5937\nEpoch [4/15], Step [7/106], Loss : 2.5923\nEpoch [4/15], Step [8/106], Loss : 2.5515\nEpoch [4/15], Step [9/106], Loss : 2.5781\nEpoch [4/15], Step [10/106], Loss : 2.5710\nEpoch [4/15], Step [11/106], Loss : 2.5662\nEpoch [4/15], Step [12/106], Loss : 2.5818\nEpoch [4/15], Step [13/106], Loss : 2.5644\nEpoch [4/15], Step [14/106], Loss : 2.5682\nEpoch [4/15], Step [15/106], Loss : 2.5510\nEpoch [4/15], Step [16/106], Loss : 2.5564\nEpoch [4/15], Step [17/106], Loss : 2.5573\nEpoch [4/15], Step [18/106], Loss : 2.5574\nEpoch [4/15], Step [19/106], Loss : 2.5564\nEpoch [4/15], Step [20/106], Loss : 2.5332\nEpoch [4/15], Step [21/106], Loss : 2.5395\nEpoch [4/15], Step [22/106], Loss : 2.5282\nEpoch [4/15], Step [23/106], Loss : 2.5203\nEpoch [4/15], Step [24/106], Loss : 2.5187\nEpoch [4/15], Step [25/106], Loss : 2.5197\nEpoch [4/15], Step [26/106], Loss : 2.5087\nEpoch [4/15], Step [27/106], Loss : 2.5091\nEpoch [4/15], Step [28/106], Loss : 2.5175\nEpoch [4/15], Step [29/106], Loss : 2.5060\nEpoch [4/15], Step [30/106], Loss : 2.5111\nEpoch [4/15], Step [31/106], Loss : 2.5147\nEpoch [4/15], Step [32/106], Loss : 2.5214\nEpoch [4/15], Step [33/106], Loss : 2.5248\nEpoch [4/15], Step [34/106], Loss : 2.5258\nEpoch [4/15], Step [35/106], Loss : 2.5230\nEpoch [4/15], Step [36/106], Loss : 2.5252\nEpoch [4/15], Step [37/106], Loss : 2.5310\nEpoch [4/15], Step [38/106], Loss : 2.5321\nEpoch [4/15], Step [39/106], Loss : 2.5316\nEpoch [4/15], Step [40/106], Loss : 2.5383\nEpoch [4/15], Step [41/106], Loss : 2.5439\nEpoch [4/15], Step [42/106], Loss : 2.5466\nEpoch [4/15], Step [43/106], Loss : 2.5503\nEpoch [4/15], Step [44/106], Loss : 2.5541\nEpoch [4/15], Step [45/106], Loss : 2.5526\nEpoch [4/15], Step [46/106], Loss : 2.5541\nEpoch [4/15], Step [47/106], Loss : 2.5532\nEpoch [4/15], Step [48/106], Loss : 2.5578\nEpoch [4/15], Step [49/106], Loss : 2.5631\nEpoch [4/15], Step [50/106], Loss : 2.5684\nEpoch [4/15], Step [51/106], Loss : 2.5716\nEpoch [4/15], Step [52/106], Loss : 2.5704\nEpoch [4/15], Step [53/106], Loss : 2.5657\nEpoch [4/15], Step [54/106], Loss : 2.5661\nEpoch [4/15], Step [55/106], Loss : 2.5704\nEpoch [4/15], Step [56/106], Loss : 2.5695\nEpoch [4/15], Step [57/106], Loss : 2.5604\nEpoch [4/15], Step [58/106], Loss : 2.5599\nEpoch [4/15], Step [59/106], Loss : 2.5573\nEpoch [4/15], Step [60/106], Loss : 2.5569\nEpoch [4/15], Step [61/106], Loss : 2.5537\nEpoch [4/15], Step [62/106], Loss : 2.5609\nEpoch [4/15], Step [63/106], Loss : 2.5603\nEpoch [4/15], Step [64/106], Loss : 2.5585\nEpoch [4/15], Step [65/106], Loss : 2.5556\nEpoch [4/15], Step [66/106], Loss : 2.5537\nEpoch [4/15], Step [67/106], Loss : 2.5548\nEpoch [4/15], Step [68/106], Loss : 2.5564\nEpoch [4/15], Step [69/106], Loss : 2.5592\nEpoch [4/15], Step [70/106], Loss : 2.5586\nEpoch [4/15], Step [71/106], Loss : 2.5622\nEpoch [4/15], Step [72/106], Loss : 2.5617\nEpoch [4/15], Step [73/106], Loss : 2.5616\nEpoch [4/15], Step [74/106], Loss : 2.5606\nEpoch [4/15], Step [75/106], Loss : 2.5566\nEpoch [4/15], Step [76/106], Loss : 2.5524\nEpoch [4/15], Step [77/106], Loss : 2.5539\nEpoch [4/15], Step [78/106], Loss : 2.5522\nEpoch [4/15], Step [79/106], Loss : 2.5514\nEpoch [4/15], Step [80/106], Loss : 2.5498\nEpoch [4/15], Step [81/106], Loss : 2.5467\nEpoch [4/15], Step [82/106], Loss : 2.5403\nEpoch [4/15], Step [83/106], Loss : 2.5390\nEpoch [4/15], Step [84/106], Loss : 2.5403\nEpoch [4/15], Step [85/106], Loss : 2.5452\nEpoch [4/15], Step [86/106], Loss : 2.5434\nEpoch [4/15], Step [87/106], Loss : 2.5465\nEpoch [4/15], Step [88/106], Loss : 2.5496\nEpoch [4/15], Step [89/106], Loss : 2.5529\nEpoch [4/15], Step [90/106], Loss : 2.5519\nEpoch [4/15], Step [91/106], Loss : 2.5531\nEpoch [4/15], Step [92/106], Loss : 2.5486\nEpoch [4/15], Step [93/106], Loss : 2.5509\nEpoch [4/15], Step [94/106], Loss : 2.5536\nEpoch [4/15], Step [95/106], Loss : 2.5544\nEpoch [4/15], Step [96/106], Loss : 2.5575\nEpoch [4/15], Step [97/106], Loss : 2.5577\nEpoch [4/15], Step [98/106], Loss : 2.5601\nEpoch [4/15], Step [99/106], Loss : 2.5604\nEpoch [4/15], Step [100/106], Loss : 2.5613\nEpoch [4/15], Step [101/106], Loss : 2.5585\nEpoch [4/15], Step [102/106], Loss : 2.5584\nEpoch [4/15], Step [103/106], Loss : 2.5607\nEpoch [4/15], Step [104/106], Loss : 2.5586\nEpoch [4/15], Step [105/106], Loss : 2.5588\nEpoch [4/15], Step [106/106], Loss : 2.5586\nEpoch: 4, Loss: 2.5586, Accuracy: 0.4299\nAccuracy of the network val voices: 43.3166 %\nSaving best model!\nEpoch [5/15], Step [1/106], Loss : 2.3427\nEpoch [5/15], Step [2/106], Loss : 2.6278\nEpoch [5/15], Step [3/106], Loss : 2.6505\nEpoch [5/15], Step [4/106], Loss : 2.5887\nEpoch [5/15], Step [5/106], Loss : 2.5501\nEpoch [5/15], Step [6/106], Loss : 2.5381\nEpoch [5/15], Step [7/106], Loss : 2.5036\nEpoch [5/15], Step [8/106], Loss : 2.4796\nEpoch [5/15], Step [9/106], Loss : 2.4902\nEpoch [5/15], Step [10/106], Loss : 2.4810\nEpoch [5/15], Step [11/106], Loss : 2.4652\nEpoch [5/15], Step [12/106], Loss : 2.4711\nEpoch [5/15], Step [13/106], Loss : 2.4571\nEpoch [5/15], Step [14/106], Loss : 2.4542\nEpoch [5/15], Step [15/106], Loss : 2.4407\nEpoch [5/15], Step [16/106], Loss : 2.4300\nEpoch [5/15], Step [17/106], Loss : 2.4181\nEpoch [5/15], Step [18/106], Loss : 2.4024\nEpoch [5/15], Step [19/106], Loss : 2.4125\nEpoch [5/15], Step [20/106], Loss : 2.4008\nEpoch [5/15], Step [21/106], Loss : 2.3940\nEpoch [5/15], Step [22/106], Loss : 2.3879\nEpoch [5/15], Step [23/106], Loss : 2.3809\nEpoch [5/15], Step [24/106], Loss : 2.3737\nEpoch [5/15], Step [25/106], Loss : 2.3633\nEpoch [5/15], Step [26/106], Loss : 2.3647\nEpoch [5/15], Step [27/106], Loss : 2.3505\nEpoch [5/15], Step [28/106], Loss : 2.3487\nEpoch [5/15], Step [29/106], Loss : 2.3531\nEpoch [5/15], Step [30/106], Loss : 2.3406\nEpoch [5/15], Step [31/106], Loss : 2.3472\nEpoch [5/15], Step [32/106], Loss : 2.3477\nEpoch [5/15], Step [33/106], Loss : 2.3379\nEpoch [5/15], Step [34/106], Loss : 2.3252\nEpoch [5/15], Step [35/106], Loss : 2.3156\nEpoch [5/15], Step [36/106], Loss : 2.3077\nEpoch [5/15], Step [37/106], Loss : 2.3111\nEpoch [5/15], Step [38/106], Loss : 2.3079\nEpoch [5/15], Step [39/106], Loss : 2.2974\nEpoch [5/15], Step [40/106], Loss : 2.2893\nEpoch [5/15], Step [41/106], Loss : 2.2998\nEpoch [5/15], Step [42/106], Loss : 2.2967\nEpoch [5/15], Step [43/106], Loss : 2.2940\nEpoch [5/15], Step [44/106], Loss : 2.3043\nEpoch [5/15], Step [45/106], Loss : 2.3019\nEpoch [5/15], Step [46/106], Loss : 2.2995\nEpoch [5/15], Step [47/106], Loss : 2.3068\nEpoch [5/15], Step [48/106], Loss : 2.3061\nEpoch [5/15], Step [49/106], Loss : 2.3056\nEpoch [5/15], Step [50/106], Loss : 2.3088\nEpoch [5/15], Step [51/106], Loss : 2.3091\nEpoch [5/15], Step [52/106], Loss : 2.3106\nEpoch [5/15], Step [53/106], Loss : 2.3093\nEpoch [5/15], Step [54/106], Loss : 2.3108\nEpoch [5/15], Step [55/106], Loss : 2.3115\nEpoch [5/15], Step [56/106], Loss : 2.3142\nEpoch [5/15], Step [57/106], Loss : 2.3122\nEpoch [5/15], Step [58/106], Loss : 2.3124\nEpoch [5/15], Step [59/106], Loss : 2.3177\nEpoch [5/15], Step [60/106], Loss : 2.3169\nEpoch [5/15], Step [61/106], Loss : 2.3193\nEpoch [5/15], Step [62/106], Loss : 2.3164\nEpoch [5/15], Step [63/106], Loss : 2.3185\nEpoch [5/15], Step [64/106], Loss : 2.3136\nEpoch [5/15], Step [65/106], Loss : 2.3139\nEpoch [5/15], Step [66/106], Loss : 2.3146\nEpoch [5/15], Step [67/106], Loss : 2.3123\nEpoch [5/15], Step [68/106], Loss : 2.3092\nEpoch [5/15], Step [69/106], Loss : 2.3036\nEpoch [5/15], Step [70/106], Loss : 2.3001\nEpoch [5/15], Step [71/106], Loss : 2.3010\nEpoch [5/15], Step [72/106], Loss : 2.2974\nEpoch [5/15], Step [73/106], Loss : 2.2962\nEpoch [5/15], Step [74/106], Loss : 2.2934\nEpoch [5/15], Step [75/106], Loss : 2.2909\nEpoch [5/15], Step [76/106], Loss : 2.2889\nEpoch [5/15], Step [77/106], Loss : 2.2877\nEpoch [5/15], Step [78/106], Loss : 2.2885\nEpoch [5/15], Step [79/106], Loss : 2.2873\nEpoch [5/15], Step [80/106], Loss : 2.2821\nEpoch [5/15], Step [81/106], Loss : 2.2809\nEpoch [5/15], Step [82/106], Loss : 2.2814\nEpoch [5/15], Step [83/106], Loss : 2.2814\nEpoch [5/15], Step [84/106], Loss : 2.2807\nEpoch [5/15], Step [85/106], Loss : 2.2793\nEpoch [5/15], Step [86/106], Loss : 2.2758\nEpoch [5/15], Step [87/106], Loss : 2.2757\nEpoch [5/15], Step [88/106], Loss : 2.2780\nEpoch [5/15], Step [89/106], Loss : 2.2762\nEpoch [5/15], Step [90/106], Loss : 2.2774\nEpoch [5/15], Step [91/106], Loss : 2.2732\nEpoch [5/15], Step [92/106], Loss : 2.2716\nEpoch [5/15], Step [93/106], Loss : 2.2685\nEpoch [5/15], Step [94/106], Loss : 2.2679\nEpoch [5/15], Step [95/106], Loss : 2.2687\nEpoch [5/15], Step [96/106], Loss : 2.2676\nEpoch [5/15], Step [97/106], Loss : 2.2669\nEpoch [5/15], Step [98/106], Loss : 2.2659\nEpoch [5/15], Step [99/106], Loss : 2.2644\nEpoch [5/15], Step [100/106], Loss : 2.2600\nEpoch [5/15], Step [101/106], Loss : 2.2585\nEpoch [5/15], Step [102/106], Loss : 2.2588\nEpoch [5/15], Step [103/106], Loss : 2.2589\nEpoch [5/15], Step [104/106], Loss : 2.2581\nEpoch [5/15], Step [105/106], Loss : 2.2569\nEpoch [5/15], Step [106/106], Loss : 2.2562\nEpoch: 5, Loss: 2.2562, Accuracy: 0.4822\nAccuracy of the network val voices: 48.8640 %\nSaving best model!\nEpoch [6/15], Step [1/106], Loss : 1.7596\nEpoch [6/15], Step [2/106], Loss : 1.6917\nEpoch [6/15], Step [3/106], Loss : 1.8709\nEpoch [6/15], Step [4/106], Loss : 1.8927\nEpoch [6/15], Step [5/106], Loss : 1.8783\nEpoch [6/15], Step [6/106], Loss : 1.8761\nEpoch [6/15], Step [7/106], Loss : 1.8535\nEpoch [6/15], Step [8/106], Loss : 1.8591\nEpoch [6/15], Step [9/106], Loss : 1.8478\nEpoch [6/15], Step [10/106], Loss : 1.8779\nEpoch [6/15], Step [11/106], Loss : 1.8809\nEpoch [6/15], Step [12/106], Loss : 1.9030\nEpoch [6/15], Step [13/106], Loss : 1.9181\nEpoch [6/15], Step [14/106], Loss : 1.9215\nEpoch [6/15], Step [15/106], Loss : 1.9440\nEpoch [6/15], Step [16/106], Loss : 1.9630\nEpoch [6/15], Step [17/106], Loss : 1.9555\nEpoch [6/15], Step [18/106], Loss : 1.9694\nEpoch [6/15], Step [19/106], Loss : 1.9635\nEpoch [6/15], Step [20/106], Loss : 1.9768\nEpoch [6/15], Step [21/106], Loss : 1.9760\nEpoch [6/15], Step [22/106], Loss : 1.9596\nEpoch [6/15], Step [23/106], Loss : 1.9565\nEpoch [6/15], Step [24/106], Loss : 1.9665\nEpoch [6/15], Step [25/106], Loss : 1.9718\nEpoch [6/15], Step [26/106], Loss : 1.9701\nEpoch [6/15], Step [27/106], Loss : 1.9653\nEpoch [6/15], Step [28/106], Loss : 1.9720\nEpoch [6/15], Step [29/106], Loss : 1.9750\nEpoch [6/15], Step [30/106], Loss : 1.9748\nEpoch [6/15], Step [31/106], Loss : 1.9711\nEpoch [6/15], Step [32/106], Loss : 1.9714\nEpoch [6/15], Step [33/106], Loss : 1.9782\nEpoch [6/15], Step [34/106], Loss : 1.9776\nEpoch [6/15], Step [35/106], Loss : 1.9772\nEpoch [6/15], Step [36/106], Loss : 1.9800\nEpoch [6/15], Step [37/106], Loss : 1.9835\nEpoch [6/15], Step [38/106], Loss : 1.9841\nEpoch [6/15], Step [39/106], Loss : 1.9783\nEpoch [6/15], Step [40/106], Loss : 1.9821\nEpoch [6/15], Step [41/106], Loss : 1.9802\nEpoch [6/15], Step [42/106], Loss : 1.9835\nEpoch [6/15], Step [43/106], Loss : 1.9797\nEpoch [6/15], Step [44/106], Loss : 1.9729\nEpoch [6/15], Step [45/106], Loss : 1.9670\nEpoch [6/15], Step [46/106], Loss : 1.9638\nEpoch [6/15], Step [47/106], Loss : 1.9597\nEpoch [6/15], Step [48/106], Loss : 1.9565\nEpoch [6/15], Step [49/106], Loss : 1.9470\nEpoch [6/15], Step [50/106], Loss : 1.9432\nEpoch [6/15], Step [51/106], Loss : 1.9346\nEpoch [6/15], Step [52/106], Loss : 1.9296\nEpoch [6/15], Step [53/106], Loss : 1.9275\nEpoch [6/15], Step [54/106], Loss : 1.9256\nEpoch [6/15], Step [55/106], Loss : 1.9166\nEpoch [6/15], Step [56/106], Loss : 1.9174\nEpoch [6/15], Step [57/106], Loss : 1.9146\nEpoch [6/15], Step [58/106], Loss : 1.9142\nEpoch [6/15], Step [59/106], Loss : 1.9102\nEpoch [6/15], Step [60/106], Loss : 1.9096\nEpoch [6/15], Step [61/106], Loss : 1.9110\nEpoch [6/15], Step [62/106], Loss : 1.9176\nEpoch [6/15], Step [63/106], Loss : 1.9186\nEpoch [6/15], Step [64/106], Loss : 1.9191\nEpoch [6/15], Step [65/106], Loss : 1.9199\nEpoch [6/15], Step [66/106], Loss : 1.9169\nEpoch [6/15], Step [67/106], Loss : 1.9166\nEpoch [6/15], Step [68/106], Loss : 1.9113\nEpoch [6/15], Step [69/106], Loss : 1.9092\nEpoch [6/15], Step [70/106], Loss : 1.9111\nEpoch [6/15], Step [71/106], Loss : 1.9109\nEpoch [6/15], Step [72/106], Loss : 1.9111\nEpoch [6/15], Step [73/106], Loss : 1.9084\nEpoch [6/15], Step [74/106], Loss : 1.9065\nEpoch [6/15], Step [75/106], Loss : 1.9088\nEpoch [6/15], Step [76/106], Loss : 1.9065\nEpoch [6/15], Step [77/106], Loss : 1.9085\nEpoch [6/15], Step [78/106], Loss : 1.9079\nEpoch [6/15], Step [79/106], Loss : 1.9043\nEpoch [6/15], Step [80/106], Loss : 1.9033\nEpoch [6/15], Step [81/106], Loss : 1.9034\nEpoch [6/15], Step [82/106], Loss : 1.9043\nEpoch [6/15], Step [83/106], Loss : 1.9054\nEpoch [6/15], Step [84/106], Loss : 1.9049\nEpoch [6/15], Step [85/106], Loss : 1.9054\nEpoch [6/15], Step [86/106], Loss : 1.9030\nEpoch [6/15], Step [87/106], Loss : 1.9074\nEpoch [6/15], Step [88/106], Loss : 1.9077\nEpoch [6/15], Step [89/106], Loss : 1.9072\nEpoch [6/15], Step [90/106], Loss : 1.9075\nEpoch [6/15], Step [91/106], Loss : 1.9075\nEpoch [6/15], Step [92/106], Loss : 1.9060\nEpoch [6/15], Step [93/106], Loss : 1.9055\nEpoch [6/15], Step [94/106], Loss : 1.9086\nEpoch [6/15], Step [95/106], Loss : 1.9067\nEpoch [6/15], Step [96/106], Loss : 1.9070\nEpoch [6/15], Step [97/106], Loss : 1.9039\nEpoch [6/15], Step [98/106], Loss : 1.9015\nEpoch [6/15], Step [99/106], Loss : 1.9006\nEpoch [6/15], Step [100/106], Loss : 1.9016\nEpoch [6/15], Step [101/106], Loss : 1.8995\nEpoch [6/15], Step [102/106], Loss : 1.8973\nEpoch [6/15], Step [103/106], Loss : 1.8964\nEpoch [6/15], Step [104/106], Loss : 1.8952\nEpoch [6/15], Step [105/106], Loss : 1.8933\nEpoch [6/15], Step [106/106], Loss : 1.8935\nEpoch: 6, Loss: 1.8935, Accuracy: 0.5522\nAccuracy of the network val voices: 55.0900 %\nSaving best model!\nEpoch [7/15], Step [1/106], Loss : 1.6445\nEpoch [7/15], Step [2/106], Loss : 1.5999\nEpoch [7/15], Step [3/106], Loss : 1.6377\nEpoch [7/15], Step [4/106], Loss : 1.7338\nEpoch [7/15], Step [5/106], Loss : 1.6841\nEpoch [7/15], Step [6/106], Loss : 1.6304\nEpoch [7/15], Step [7/106], Loss : 1.6204\nEpoch [7/15], Step [8/106], Loss : 1.5930\nEpoch [7/15], Step [9/106], Loss : 1.5914\nEpoch [7/15], Step [10/106], Loss : 1.5807\nEpoch [7/15], Step [11/106], Loss : 1.5748\nEpoch [7/15], Step [12/106], Loss : 1.5794\nEpoch [7/15], Step [13/106], Loss : 1.5765\nEpoch [7/15], Step [14/106], Loss : 1.5673\nEpoch [7/15], Step [15/106], Loss : 1.5916\nEpoch [7/15], Step [16/106], Loss : 1.5936\nEpoch [7/15], Step [17/106], Loss : 1.5838\nEpoch [7/15], Step [18/106], Loss : 1.5948\nEpoch [7/15], Step [19/106], Loss : 1.6005\nEpoch [7/15], Step [20/106], Loss : 1.6164\nEpoch [7/15], Step [21/106], Loss : 1.6109\nEpoch [7/15], Step [22/106], Loss : 1.6094\nEpoch [7/15], Step [23/106], Loss : 1.6063\nEpoch [7/15], Step [24/106], Loss : 1.6167\nEpoch [7/15], Step [25/106], Loss : 1.6098\nEpoch [7/15], Step [26/106], Loss : 1.6136\nEpoch [7/15], Step [27/106], Loss : 1.6015\nEpoch [7/15], Step [28/106], Loss : 1.5929\nEpoch [7/15], Step [29/106], Loss : 1.5990\nEpoch [7/15], Step [30/106], Loss : 1.5961\nEpoch [7/15], Step [31/106], Loss : 1.5855\nEpoch [7/15], Step [32/106], Loss : 1.5875\nEpoch [7/15], Step [33/106], Loss : 1.5801\nEpoch [7/15], Step [34/106], Loss : 1.5736\nEpoch [7/15], Step [35/106], Loss : 1.5731\nEpoch [7/15], Step [36/106], Loss : 1.5715\nEpoch [7/15], Step [37/106], Loss : 1.5714\nEpoch [7/15], Step [38/106], Loss : 1.5651\nEpoch [7/15], Step [39/106], Loss : 1.5653\nEpoch [7/15], Step [40/106], Loss : 1.5714\nEpoch [7/15], Step [41/106], Loss : 1.5681\nEpoch [7/15], Step [42/106], Loss : 1.5703\nEpoch [7/15], Step [43/106], Loss : 1.5788\nEpoch [7/15], Step [44/106], Loss : 1.5789\nEpoch [7/15], Step [45/106], Loss : 1.5855\nEpoch [7/15], Step [46/106], Loss : 1.5901\nEpoch [7/15], Step [47/106], Loss : 1.5880\nEpoch [7/15], Step [48/106], Loss : 1.5904\nEpoch [7/15], Step [49/106], Loss : 1.5892\nEpoch [7/15], Step [50/106], Loss : 1.5877\nEpoch [7/15], Step [51/106], Loss : 1.5921\nEpoch [7/15], Step [52/106], Loss : 1.5967\nEpoch [7/15], Step [53/106], Loss : 1.5954\nEpoch [7/15], Step [54/106], Loss : 1.5935\nEpoch [7/15], Step [55/106], Loss : 1.5924\nEpoch [7/15], Step [56/106], Loss : 1.5918\nEpoch [7/15], Step [57/106], Loss : 1.5946\nEpoch [7/15], Step [58/106], Loss : 1.5982\nEpoch [7/15], Step [59/106], Loss : 1.5975\nEpoch [7/15], Step [60/106], Loss : 1.5989\nEpoch [7/15], Step [61/106], Loss : 1.6018\nEpoch [7/15], Step [62/106], Loss : 1.6010\nEpoch [7/15], Step [63/106], Loss : 1.5969\nEpoch [7/15], Step [64/106], Loss : 1.5927\nEpoch [7/15], Step [65/106], Loss : 1.5970\nEpoch [7/15], Step [66/106], Loss : 1.5982\nEpoch [7/15], Step [67/106], Loss : 1.5973\nEpoch [7/15], Step [68/106], Loss : 1.5963\nEpoch [7/15], Step [69/106], Loss : 1.5968\nEpoch [7/15], Step [70/106], Loss : 1.5978\nEpoch [7/15], Step [71/106], Loss : 1.5955\nEpoch [7/15], Step [72/106], Loss : 1.5965\nEpoch [7/15], Step [73/106], Loss : 1.5949\nEpoch [7/15], Step [74/106], Loss : 1.5969\nEpoch [7/15], Step [75/106], Loss : 1.5944\nEpoch [7/15], Step [76/106], Loss : 1.5964\nEpoch [7/15], Step [77/106], Loss : 1.5958\nEpoch [7/15], Step [78/106], Loss : 1.5942\nEpoch [7/15], Step [79/106], Loss : 1.5965\nEpoch [7/15], Step [80/106], Loss : 1.5996\nEpoch [7/15], Step [81/106], Loss : 1.5985\nEpoch [7/15], Step [82/106], Loss : 1.6003\nEpoch [7/15], Step [83/106], Loss : 1.5977\nEpoch [7/15], Step [84/106], Loss : 1.6009\nEpoch [7/15], Step [85/106], Loss : 1.6021\nEpoch [7/15], Step [86/106], Loss : 1.6006\nEpoch [7/15], Step [87/106], Loss : 1.5993\nEpoch [7/15], Step [88/106], Loss : 1.6008\nEpoch [7/15], Step [89/106], Loss : 1.6010\nEpoch [7/15], Step [90/106], Loss : 1.6000\nEpoch [7/15], Step [91/106], Loss : 1.6030\nEpoch [7/15], Step [92/106], Loss : 1.6037\nEpoch [7/15], Step [93/106], Loss : 1.6022\nEpoch [7/15], Step [94/106], Loss : 1.6059\nEpoch [7/15], Step [95/106], Loss : 1.6083\nEpoch [7/15], Step [96/106], Loss : 1.6059\nEpoch [7/15], Step [97/106], Loss : 1.6035\nEpoch [7/15], Step [98/106], Loss : 1.6038\nEpoch [7/15], Step [99/106], Loss : 1.6058\nEpoch [7/15], Step [100/106], Loss : 1.6078\nEpoch [7/15], Step [101/106], Loss : 1.6083\nEpoch [7/15], Step [102/106], Loss : 1.6100\nEpoch [7/15], Step [103/106], Loss : 1.6108\nEpoch [7/15], Step [104/106], Loss : 1.6071\nEpoch [7/15], Step [105/106], Loss : 1.6054\nEpoch [7/15], Step [106/106], Loss : 1.6043\nEpoch: 7, Loss: 1.6043, Accuracy: 0.6024\nAccuracy of the network val voices: 57.6571 %\nSaving best model!\nEpoch [8/15], Step [1/106], Loss : 1.3274\nEpoch [8/15], Step [2/106], Loss : 1.4840\nEpoch [8/15], Step [3/106], Loss : 1.4986\nEpoch [8/15], Step [4/106], Loss : 1.4038\nEpoch [8/15], Step [5/106], Loss : 1.3497\nEpoch [8/15], Step [6/106], Loss : 1.3546\nEpoch [8/15], Step [7/106], Loss : 1.3382\nEpoch [8/15], Step [8/106], Loss : 1.3102\nEpoch [8/15], Step [9/106], Loss : 1.2900\nEpoch [8/15], Step [10/106], Loss : 1.2971\nEpoch [8/15], Step [11/106], Loss : 1.3084\nEpoch [8/15], Step [12/106], Loss : 1.2917\nEpoch [8/15], Step [13/106], Loss : 1.3032\nEpoch [8/15], Step [14/106], Loss : 1.3151\nEpoch [8/15], Step [15/106], Loss : 1.3207\nEpoch [8/15], Step [16/106], Loss : 1.3240\nEpoch [8/15], Step [17/106], Loss : 1.3306\nEpoch [8/15], Step [18/106], Loss : 1.3393\nEpoch [8/15], Step [19/106], Loss : 1.3273\nEpoch [8/15], Step [20/106], Loss : 1.3219\nEpoch [8/15], Step [21/106], Loss : 1.3225\nEpoch [8/15], Step [22/106], Loss : 1.3269\nEpoch [8/15], Step [23/106], Loss : 1.3259\nEpoch [8/15], Step [24/106], Loss : 1.3211\nEpoch [8/15], Step [25/106], Loss : 1.3270\nEpoch [8/15], Step [26/106], Loss : 1.3312\nEpoch [8/15], Step [27/106], Loss : 1.3308\nEpoch [8/15], Step [28/106], Loss : 1.3386\nEpoch [8/15], Step [29/106], Loss : 1.3401\nEpoch [8/15], Step [30/106], Loss : 1.3452\nEpoch [8/15], Step [31/106], Loss : 1.3580\nEpoch [8/15], Step [32/106], Loss : 1.3533\nEpoch [8/15], Step [33/106], Loss : 1.3533\nEpoch [8/15], Step [34/106], Loss : 1.3580\nEpoch [8/15], Step [35/106], Loss : 1.3580\nEpoch [8/15], Step [36/106], Loss : 1.3579\nEpoch [8/15], Step [37/106], Loss : 1.3596\nEpoch [8/15], Step [38/106], Loss : 1.3546\nEpoch [8/15], Step [39/106], Loss : 1.3541\nEpoch [8/15], Step [40/106], Loss : 1.3622\nEpoch [8/15], Step [41/106], Loss : 1.3623\nEpoch [8/15], Step [42/106], Loss : 1.3657\nEpoch [8/15], Step [43/106], Loss : 1.3640\nEpoch [8/15], Step [44/106], Loss : 1.3592\nEpoch [8/15], Step [45/106], Loss : 1.3593\nEpoch [8/15], Step [46/106], Loss : 1.3603\nEpoch [8/15], Step [47/106], Loss : 1.3535\nEpoch [8/15], Step [48/106], Loss : 1.3513\nEpoch [8/15], Step [49/106], Loss : 1.3548\nEpoch [8/15], Step [50/106], Loss : 1.3579\nEpoch [8/15], Step [51/106], Loss : 1.3548\nEpoch [8/15], Step [52/106], Loss : 1.3511\nEpoch [8/15], Step [53/106], Loss : 1.3481\nEpoch [8/15], Step [54/106], Loss : 1.3539\nEpoch [8/15], Step [55/106], Loss : 1.3515\nEpoch [8/15], Step [56/106], Loss : 1.3536\nEpoch [8/15], Step [57/106], Loss : 1.3526\nEpoch [8/15], Step [58/106], Loss : 1.3554\nEpoch [8/15], Step [59/106], Loss : 1.3518\nEpoch [8/15], Step [60/106], Loss : 1.3489\nEpoch [8/15], Step [61/106], Loss : 1.3453\nEpoch [8/15], Step [62/106], Loss : 1.3407\nEpoch [8/15], Step [63/106], Loss : 1.3464\nEpoch [8/15], Step [64/106], Loss : 1.3516\nEpoch [8/15], Step [65/106], Loss : 1.3517\nEpoch [8/15], Step [66/106], Loss : 1.3502\nEpoch [8/15], Step [67/106], Loss : 1.3507\nEpoch [8/15], Step [68/106], Loss : 1.3471\nEpoch [8/15], Step [69/106], Loss : 1.3468\nEpoch [8/15], Step [70/106], Loss : 1.3493\nEpoch [8/15], Step [71/106], Loss : 1.3489\nEpoch [8/15], Step [72/106], Loss : 1.3539\nEpoch [8/15], Step [73/106], Loss : 1.3552\nEpoch [8/15], Step [74/106], Loss : 1.3609\nEpoch [8/15], Step [75/106], Loss : 1.3624\nEpoch [8/15], Step [76/106], Loss : 1.3657\nEpoch [8/15], Step [77/106], Loss : 1.3681\nEpoch [8/15], Step [78/106], Loss : 1.3688\nEpoch [8/15], Step [79/106], Loss : 1.3729\nEpoch [8/15], Step [80/106], Loss : 1.3691\nEpoch [8/15], Step [81/106], Loss : 1.3678\nEpoch [8/15], Step [82/106], Loss : 1.3684\nEpoch [8/15], Step [83/106], Loss : 1.3672\nEpoch [8/15], Step [84/106], Loss : 1.3683\nEpoch [8/15], Step [85/106], Loss : 1.3682\nEpoch [8/15], Step [86/106], Loss : 1.3680\nEpoch [8/15], Step [87/106], Loss : 1.3709\nEpoch [8/15], Step [88/106], Loss : 1.3709\nEpoch [8/15], Step [89/106], Loss : 1.3712\nEpoch [8/15], Step [90/106], Loss : 1.3724\nEpoch [8/15], Step [91/106], Loss : 1.3702\nEpoch [8/15], Step [92/106], Loss : 1.3678\nEpoch [8/15], Step [93/106], Loss : 1.3666\nEpoch [8/15], Step [94/106], Loss : 1.3666\nEpoch [8/15], Step [95/106], Loss : 1.3669\nEpoch [8/15], Step [96/106], Loss : 1.3713\nEpoch [8/15], Step [97/106], Loss : 1.3704\nEpoch [8/15], Step [98/106], Loss : 1.3707\nEpoch [8/15], Step [99/106], Loss : 1.3709\nEpoch [8/15], Step [100/106], Loss : 1.3737\nEpoch [8/15], Step [101/106], Loss : 1.3731\nEpoch [8/15], Step [102/106], Loss : 1.3742\nEpoch [8/15], Step [103/106], Loss : 1.3732\nEpoch [8/15], Step [104/106], Loss : 1.3734\nEpoch [8/15], Step [105/106], Loss : 1.3739\nEpoch [8/15], Step [106/106], Loss : 1.3739\nEpoch: 8, Loss: 1.3739, Accuracy: 0.6592\nAccuracy of the network val voices: 59.3685 %\nSaving best model!\nEpoch [9/15], Step [1/106], Loss : 1.2194\nEpoch [9/15], Step [2/106], Loss : 1.1486\nEpoch [9/15], Step [3/106], Loss : 1.1047\nEpoch [9/15], Step [4/106], Loss : 1.1271\nEpoch [9/15], Step [5/106], Loss : 1.1593\nEpoch [9/15], Step [6/106], Loss : 1.1669\nEpoch [9/15], Step [7/106], Loss : 1.1712\nEpoch [9/15], Step [8/106], Loss : 1.1640\nEpoch [9/15], Step [9/106], Loss : 1.1577\nEpoch [9/15], Step [10/106], Loss : 1.1717\nEpoch [9/15], Step [11/106], Loss : 1.1733\nEpoch [9/15], Step [12/106], Loss : 1.1860\nEpoch [9/15], Step [13/106], Loss : 1.1804\nEpoch [9/15], Step [14/106], Loss : 1.1615\nEpoch [9/15], Step [15/106], Loss : 1.1568\nEpoch [9/15], Step [16/106], Loss : 1.1528\nEpoch [9/15], Step [17/106], Loss : 1.1382\nEpoch [9/15], Step [22/106], Loss : 1.1215\nEpoch [9/15], Step [23/106], Loss : 1.1176\nEpoch [9/15], Step [24/106], Loss : 1.1211\nEpoch [9/15], Step [25/106], Loss : 1.1209\nEpoch [9/15], Step [26/106], Loss : 1.1157\nEpoch [9/15], Step [27/106], Loss : 1.1207\nEpoch [9/15], Step [28/106], Loss : 1.1252\nEpoch [9/15], Step [29/106], Loss : 1.1243\nEpoch [9/15], Step [30/106], Loss : 1.1252\nEpoch [9/15], Step [31/106], Loss : 1.1247\nEpoch [9/15], Step [32/106], Loss : 1.1242\nEpoch [9/15], Step [33/106], Loss : 1.1251\nEpoch [9/15], Step [34/106], Loss : 1.1224\nEpoch [9/15], Step [35/106], Loss : 1.1331\nEpoch [9/15], Step [36/106], Loss : 1.1287\nEpoch [9/15], Step [37/106], Loss : 1.1284\nEpoch [9/15], Step [38/106], Loss : 1.1333\nEpoch [9/15], Step [39/106], Loss : 1.1354\nEpoch [9/15], Step [40/106], Loss : 1.1384\nEpoch [9/15], Step [41/106], Loss : 1.1414\nEpoch [9/15], Step [42/106], Loss : 1.1418\nEpoch [9/15], Step [43/106], Loss : 1.1510\nEpoch [9/15], Step [44/106], Loss : 1.1469\nEpoch [9/15], Step [45/106], Loss : 1.1450\nEpoch [9/15], Step [46/106], Loss : 1.1491\nEpoch [9/15], Step [47/106], Loss : 1.1505\nEpoch [9/15], Step [48/106], Loss : 1.1507\nEpoch [9/15], Step [49/106], Loss : 1.1548\nEpoch [9/15], Step [50/106], Loss : 1.1562\nEpoch [9/15], Step [51/106], Loss : 1.1601\nEpoch [9/15], Step [52/106], Loss : 1.1626\nEpoch [9/15], Step [53/106], Loss : 1.1644\nEpoch [9/15], Step [54/106], Loss : 1.1654\nEpoch [9/15], Step [55/106], Loss : 1.1670\nEpoch [9/15], Step [56/106], Loss : 1.1712\nEpoch [9/15], Step [57/106], Loss : 1.1768\nEpoch [9/15], Step [58/106], Loss : 1.1760\nEpoch [9/15], Step [59/106], Loss : 1.1788\nEpoch [9/15], Step [60/106], Loss : 1.1794\nEpoch [9/15], Step [61/106], Loss : 1.1786\nEpoch [9/15], Step [62/106], Loss : 1.1791\nEpoch [9/15], Step [63/106], Loss : 1.1787\nEpoch [9/15], Step [64/106], Loss : 1.1788\nEpoch [9/15], Step [65/106], Loss : 1.1805\nEpoch [9/15], Step [66/106], Loss : 1.1772\nEpoch [9/15], Step [67/106], Loss : 1.1752\nEpoch [9/15], Step [68/106], Loss : 1.1727\nEpoch [9/15], Step [69/106], Loss : 1.1737\nEpoch [9/15], Step [70/106], Loss : 1.1733\nEpoch [9/15], Step [71/106], Loss : 1.1752\nEpoch [9/15], Step [76/106], Loss : 1.1658\nEpoch [9/15], Step [77/106], Loss : 1.1675\nEpoch [9/15], Step [78/106], Loss : 1.1672\nEpoch [9/15], Step [79/106], Loss : 1.1672\nEpoch [9/15], Step [80/106], Loss : 1.1683\nEpoch [9/15], Step [81/106], Loss : 1.1690\nEpoch [9/15], Step [82/106], Loss : 1.1680\nEpoch [9/15], Step [83/106], Loss : 1.1697\nEpoch [9/15], Step [84/106], Loss : 1.1696\nEpoch [9/15], Step [85/106], Loss : 1.1678\nEpoch [9/15], Step [86/106], Loss : 1.1661\nEpoch [9/15], Step [87/106], Loss : 1.1706\nEpoch [9/15], Step [88/106], Loss : 1.1714\nEpoch [9/15], Step [89/106], Loss : 1.1723\nEpoch [9/15], Step [90/106], Loss : 1.1723\nEpoch [9/15], Step [91/106], Loss : 1.1737\nEpoch [9/15], Step [92/106], Loss : 1.1728\nEpoch [9/15], Step [93/106], Loss : 1.1745\nEpoch [9/15], Step [94/106], Loss : 1.1725\nEpoch [9/15], Step [95/106], Loss : 1.1725\nEpoch [9/15], Step [96/106], Loss : 1.1720\nEpoch [9/15], Step [97/106], Loss : 1.1732\nEpoch [9/15], Step [98/106], Loss : 1.1760\nEpoch [9/15], Step [99/106], Loss : 1.1769\nEpoch [9/15], Step [100/106], Loss : 1.1784\nEpoch [9/15], Step [101/106], Loss : 1.1797\nEpoch [9/15], Step [102/106], Loss : 1.1801\nEpoch [9/15], Step [103/106], Loss : 1.1798\nEpoch [9/15], Step [104/106], Loss : 1.1787\nEpoch [9/15], Step [105/106], Loss : 1.1776\nEpoch [9/15], Step [106/106], Loss : 1.1785\nEpoch: 9, Loss: 1.1785, Accuracy: 0.6923\nAccuracy of the network val voices: 62.1717 %\nSaving best model!\nEpoch [10/15], Step [1/106], Loss : 0.6835\nEpoch [10/15], Step [2/106], Loss : 0.8000\nEpoch [10/15], Step [3/106], Loss : 0.8389\nEpoch [10/15], Step [4/106], Loss : 0.8832\nEpoch [10/15], Step [5/106], Loss : 0.9177\nEpoch [10/15], Step [6/106], Loss : 0.9320\nEpoch [10/15], Step [7/106], Loss : 0.9203\nEpoch [10/15], Step [8/106], Loss : 0.9320\nEpoch [10/15], Step [9/106], Loss : 0.9388\nEpoch [10/15], Step [10/106], Loss : 0.9197\nEpoch [10/15], Step [11/106], Loss : 0.9277\nEpoch [10/15], Step [12/106], Loss : 0.9232\nEpoch [10/15], Step [13/106], Loss : 0.9175\nEpoch [10/15], Step [14/106], Loss : 0.9298\nEpoch [10/15], Step [15/106], Loss : 0.9435\nEpoch [10/15], Step [16/106], Loss : 0.9410\nEpoch [10/15], Step [17/106], Loss : 0.9319\nEpoch [10/15], Step [18/106], Loss : 0.9298\nEpoch [10/15], Step [19/106], Loss : 0.9455\nEpoch [10/15], Step [20/106], Loss : 0.9506\nEpoch [10/15], Step [21/106], Loss : 0.9539\nEpoch [10/15], Step [22/106], Loss : 0.9460\nEpoch [10/15], Step [23/106], Loss : 0.9436\nEpoch [10/15], Step [24/106], Loss : 0.9410\nEpoch [10/15], Step [25/106], Loss : 0.9524\nEpoch [10/15], Step [26/106], Loss : 0.9665\nEpoch [10/15], Step [27/106], Loss : 0.9808\nEpoch [10/15], Step [28/106], Loss : 0.9819\nEpoch [10/15], Step [29/106], Loss : 0.9834\nEpoch [10/15], Step [30/106], Loss : 0.9797\nEpoch [10/15], Step [31/106], Loss : 0.9784\nEpoch [10/15], Step [32/106], Loss : 0.9775\nEpoch [10/15], Step [33/106], Loss : 0.9787\nEpoch [10/15], Step [34/106], Loss : 0.9810\nEpoch [10/15], Step [35/106], Loss : 0.9788\nEpoch [10/15], Step [36/106], Loss : 0.9870\nEpoch [10/15], Step [37/106], Loss : 0.9943\nEpoch [10/15], Step [38/106], Loss : 0.9969\nEpoch [10/15], Step [39/106], Loss : 1.0015\nEpoch [10/15], Step [40/106], Loss : 1.0076\nEpoch [10/15], Step [41/106], Loss : 1.0083\nEpoch [10/15], Step [42/106], Loss : 1.0089\nEpoch [10/15], Step [43/106], Loss : 1.0074\nEpoch [10/15], Step [44/106], Loss : 1.0098\nEpoch [10/15], Step [45/106], Loss : 1.0083\nEpoch [10/15], Step [46/106], Loss : 1.0100\nEpoch [10/15], Step [47/106], Loss : 1.0132\nEpoch [10/15], Step [48/106], Loss : 1.0143\nEpoch [10/15], Step [49/106], Loss : 1.0125\nEpoch [10/15], Step [50/106], Loss : 1.0110\nEpoch [10/15], Step [51/106], Loss : 1.0092\nEpoch [10/15], Step [52/106], Loss : 1.0073\nEpoch [10/15], Step [53/106], Loss : 1.0084\nEpoch [10/15], Step [54/106], Loss : 1.0074\nEpoch [10/15], Step [55/106], Loss : 1.0067\nEpoch [10/15], Step [56/106], Loss : 1.0064\nEpoch [10/15], Step [57/106], Loss : 1.0079\nEpoch [10/15], Step [58/106], Loss : 1.0088\nEpoch [10/15], Step [59/106], Loss : 1.0060\nEpoch [10/15], Step [60/106], Loss : 1.0013\nEpoch [10/15], Step [61/106], Loss : 0.9998\nEpoch [10/15], Step [62/106], Loss : 1.0023\nEpoch [10/15], Step [63/106], Loss : 1.0031\nEpoch [10/15], Step [64/106], Loss : 1.0022\nEpoch [10/15], Step [65/106], Loss : 1.0018\nEpoch [10/15], Step [66/106], Loss : 1.0034\nEpoch [10/15], Step [67/106], Loss : 1.0016\nEpoch [10/15], Step [68/106], Loss : 1.0011\nEpoch [10/15], Step [69/106], Loss : 0.9969\nEpoch [10/15], Step [70/106], Loss : 0.9976\nEpoch [10/15], Step [71/106], Loss : 0.9968\nEpoch [10/15], Step [72/106], Loss : 0.9979\nEpoch [10/15], Step [73/106], Loss : 0.9979\nEpoch [10/15], Step [74/106], Loss : 0.9967\nEpoch [10/15], Step [75/106], Loss : 0.9963\nEpoch [10/15], Step [76/106], Loss : 0.9995\nEpoch [10/15], Step [77/106], Loss : 0.9981\nEpoch [10/15], Step [78/106], Loss : 0.9964\nEpoch [10/15], Step [79/106], Loss : 0.9942\nEpoch [10/15], Step [80/106], Loss : 0.9963\nEpoch [10/15], Step [81/106], Loss : 0.9984\nEpoch [10/15], Step [82/106], Loss : 0.9985\nEpoch [10/15], Step [83/106], Loss : 1.0010\nEpoch [10/15], Step [84/106], Loss : 0.9995\nEpoch [10/15], Step [85/106], Loss : 0.9966\nEpoch [10/15], Step [86/106], Loss : 0.9969\nEpoch [10/15], Step [87/106], Loss : 0.9990\nEpoch [10/15], Step [88/106], Loss : 0.9970\nEpoch [10/15], Step [89/106], Loss : 1.0004\nEpoch [10/15], Step [90/106], Loss : 0.9997\nEpoch [10/15], Step [91/106], Loss : 0.9986\nEpoch [10/15], Step [92/106], Loss : 0.9987\nEpoch [10/15], Step [93/106], Loss : 0.9993\nEpoch [10/15], Step [94/106], Loss : 0.9975\nEpoch [10/15], Step [95/106], Loss : 0.9962\nEpoch [10/15], Step [96/106], Loss : 0.9991\nEpoch [10/15], Step [97/106], Loss : 0.9984\nEpoch [10/15], Step [98/106], Loss : 0.9969\nEpoch [10/15], Step [99/106], Loss : 0.9992\nEpoch [10/15], Step [100/106], Loss : 0.9995\nEpoch [10/15], Step [101/106], Loss : 1.0001\nEpoch [10/15], Step [102/106], Loss : 0.9995\nEpoch [10/15], Step [103/106], Loss : 1.0003\nEpoch [10/15], Step [104/106], Loss : 1.0025\nEpoch [10/15], Step [105/106], Loss : 1.0037\nEpoch [10/15], Step [106/106], Loss : 1.0074\nEpoch: 10, Loss: 1.0074, Accuracy: 0.7321\nAccuracy of the network val voices: 63.1455 %\nSaving best model!\nEpoch [11/15], Step [1/106], Loss : 0.7801\nEpoch [11/15], Step [2/106], Loss : 0.7740\nEpoch [11/15], Step [3/106], Loss : 0.7734\nEpoch [11/15], Step [4/106], Loss : 0.7463\nEpoch [11/15], Step [5/106], Loss : 0.7741\nEpoch [11/15], Step [6/106], Loss : 0.7602\nEpoch [11/15], Step [7/106], Loss : 0.7925\nEpoch [11/15], Step [8/106], Loss : 0.7903\nEpoch [11/15], Step [9/106], Loss : 0.7848\nEpoch [11/15], Step [10/106], Loss : 0.7675\nEpoch [11/15], Step [11/106], Loss : 0.7898\nEpoch [11/15], Step [12/106], Loss : 0.8028\nEpoch [11/15], Step [13/106], Loss : 0.7979\nEpoch [11/15], Step [14/106], Loss : 0.8015\nEpoch [11/15], Step [15/106], Loss : 0.7994\nEpoch [11/15], Step [16/106], Loss : 0.8038\nEpoch [11/15], Step [17/106], Loss : 0.8087\nEpoch [11/15], Step [18/106], Loss : 0.8157\nEpoch [11/15], Step [19/106], Loss : 0.8275\nEpoch [11/15], Step [20/106], Loss : 0.8298\nEpoch [11/15], Step [21/106], Loss : 0.8296\nEpoch [11/15], Step [22/106], Loss : 0.8271\nEpoch [11/15], Step [23/106], Loss : 0.8199\nEpoch [11/15], Step [24/106], Loss : 0.8126\nEpoch [11/15], Step [25/106], Loss : 0.8177\nEpoch [11/15], Step [26/106], Loss : 0.8192\nEpoch [11/15], Step [27/106], Loss : 0.8175\nEpoch [11/15], Step [28/106], Loss : 0.8149\nEpoch [11/15], Step [29/106], Loss : 0.8177\nEpoch [11/15], Step [30/106], Loss : 0.8150\nEpoch [11/15], Step [31/106], Loss : 0.8181\nEpoch [11/15], Step [32/106], Loss : 0.8100\nEpoch [11/15], Step [33/106], Loss : 0.8098\nEpoch [11/15], Step [34/106], Loss : 0.8068\nEpoch [11/15], Step [35/106], Loss : 0.8033\nEpoch [11/15], Step [36/106], Loss : 0.8037\nEpoch [11/15], Step [37/106], Loss : 0.8036\nEpoch [11/15], Step [38/106], Loss : 0.8054\nEpoch [11/15], Step [39/106], Loss : 0.8052\nEpoch [11/15], Step [40/106], Loss : 0.8037\nEpoch [11/15], Step [41/106], Loss : 0.7994\nEpoch [11/15], Step [42/106], Loss : 0.7990\nEpoch [11/15], Step [43/106], Loss : 0.7973\nEpoch [11/15], Step [44/106], Loss : 0.7988\nEpoch [11/15], Step [45/106], Loss : 0.8033\nEpoch [11/15], Step [46/106], Loss : 0.8079\nEpoch [11/15], Step [47/106], Loss : 0.8064\nEpoch [11/15], Step [48/106], Loss : 0.8032\nEpoch [11/15], Step [49/106], Loss : 0.8009\nEpoch [11/15], Step [50/106], Loss : 0.8022\nEpoch [11/15], Step [51/106], Loss : 0.8044\nEpoch [11/15], Step [52/106], Loss : 0.8084\nEpoch [11/15], Step [53/106], Loss : 0.8094\nEpoch [11/15], Step [54/106], Loss : 0.8086\nEpoch [11/15], Step [55/106], Loss : 0.8087\nEpoch [11/15], Step [56/106], Loss : 0.8049\nEpoch [11/15], Step [57/106], Loss : 0.8070\nEpoch [11/15], Step [58/106], Loss : 0.8055\nEpoch [11/15], Step [59/106], Loss : 0.8008\nEpoch [11/15], Step [60/106], Loss : 0.7990\nEpoch [11/15], Step [61/106], Loss : 0.7994\nEpoch [11/15], Step [62/106], Loss : 0.8026\nEpoch [11/15], Step [63/106], Loss : 0.8029\nEpoch [11/15], Step [64/106], Loss : 0.8035\nEpoch [11/15], Step [65/106], Loss : 0.8076\nEpoch [11/15], Step [66/106], Loss : 0.8088\nEpoch [11/15], Step [67/106], Loss : 0.8130\nEpoch [11/15], Step [68/106], Loss : 0.8136\nEpoch [11/15], Step [69/106], Loss : 0.8119\nEpoch [11/15], Step [72/106], Loss : 0.8158\nEpoch [11/15], Step [73/106], Loss : 0.8163\nEpoch [11/15], Step [74/106], Loss : 0.8163\nEpoch [11/15], Step [75/106], Loss : 0.8171\nEpoch [11/15], Step [76/106], Loss : 0.8170\nEpoch [11/15], Step [77/106], Loss : 0.8161\nEpoch [11/15], Step [78/106], Loss : 0.8171\nEpoch [11/15], Step [79/106], Loss : 0.8161\nEpoch [11/15], Step [80/106], Loss : 0.8164\nEpoch [11/15], Step [81/106], Loss : 0.8151\nEpoch [11/15], Step [82/106], Loss : 0.8158\nEpoch [11/15], Step [83/106], Loss : 0.8174\nEpoch [11/15], Step [84/106], Loss : 0.8171\nEpoch [11/15], Step [85/106], Loss : 0.8179\nEpoch [11/15], Step [86/106], Loss : 0.8187\nEpoch [11/15], Step [87/106], Loss : 0.8181\nEpoch [11/15], Step [88/106], Loss : 0.8168\nEpoch [11/15], Step [89/106], Loss : 0.8154\nEpoch [11/15], Step [90/106], Loss : 0.8151\nEpoch [11/15], Step [91/106], Loss : 0.8131\nEpoch [11/15], Step [92/106], Loss : 0.8142\nEpoch [11/15], Step [93/106], Loss : 0.8148\nEpoch [11/15], Step [94/106], Loss : 0.8148\nEpoch [11/15], Step [95/106], Loss : 0.8148\nEpoch [11/15], Step [96/106], Loss : 0.8143\nEpoch [11/15], Step [97/106], Loss : 0.8145\nEpoch [11/15], Step [98/106], Loss : 0.8158\nEpoch [11/15], Step [99/106], Loss : 0.8145\nEpoch [11/15], Step [100/106], Loss : 0.8144\nEpoch [11/15], Step [101/106], Loss : 0.8147\nEpoch [11/15], Step [102/106], Loss : 0.8139\nEpoch [11/15], Step [103/106], Loss : 0.8146\nEpoch [11/15], Step [104/106], Loss : 0.8147\nEpoch [11/15], Step [105/106], Loss : 0.8150\nEpoch [11/15], Step [106/106], Loss : 0.8167\nEpoch: 11, Loss: 0.8167, Accuracy: 0.7764\nAccuracy of the network val voices: 66.3323 %\nSaving best model!\nEpoch [12/15], Step [1/106], Loss : 0.5665\nEpoch [12/15], Step [2/106], Loss : 0.6109\nEpoch [12/15], Step [3/106], Loss : 0.6877\nEpoch [12/15], Step [4/106], Loss : 0.6344\nEpoch [12/15], Step [5/106], Loss : 0.6240\nEpoch [12/15], Step [6/106], Loss : 0.6222\nEpoch [12/15], Step [7/106], Loss : 0.6386\nEpoch [12/15], Step [8/106], Loss : 0.6348\nEpoch [12/15], Step [9/106], Loss : 0.6369\nEpoch [12/15], Step [10/106], Loss : 0.6389\nEpoch [12/15], Step [11/106], Loss : 0.6249\nEpoch [12/15], Step [12/106], Loss : 0.6294\nEpoch [12/15], Step [13/106], Loss : 0.6272\nEpoch [12/15], Step [14/106], Loss : 0.6206\nEpoch [12/15], Step [15/106], Loss : 0.6263\nEpoch [12/15], Step [16/106], Loss : 0.6232\nEpoch [12/15], Step [17/106], Loss : 0.6181\nEpoch [12/15], Step [18/106], Loss : 0.6186\nEpoch [12/15], Step [19/106], Loss : 0.6239\nEpoch [12/15], Step [20/106], Loss : 0.6167\nEpoch [12/15], Step [21/106], Loss : 0.6194\nEpoch [12/15], Step [22/106], Loss : 0.6155\nEpoch [12/15], Step [23/106], Loss : 0.6180\nEpoch [12/15], Step [24/106], Loss : 0.6179\nEpoch [12/15], Step [25/106], Loss : 0.6192\nEpoch [12/15], Step [26/106], Loss : 0.6233\nEpoch [12/15], Step [27/106], Loss : 0.6168\nEpoch [12/15], Step [28/106], Loss : 0.6137\nEpoch [12/15], Step [29/106], Loss : 0.6118\nEpoch [12/15], Step [30/106], Loss : 0.6107\nEpoch [12/15], Step [31/106], Loss : 0.6127\nEpoch [12/15], Step [32/106], Loss : 0.6129\nEpoch [12/15], Step [33/106], Loss : 0.6149\nEpoch [12/15], Step [34/106], Loss : 0.6190\nEpoch [12/15], Step [35/106], Loss : 0.6249\nEpoch [12/15], Step [36/106], Loss : 0.6234\nEpoch [12/15], Step [37/106], Loss : 0.6201\nEpoch [12/15], Step [38/106], Loss : 0.6204\nEpoch [12/15], Step [39/106], Loss : 0.6238\nEpoch [12/15], Step [40/106], Loss : 0.6224\nEpoch [12/15], Step [41/106], Loss : 0.6228\nEpoch [12/15], Step [42/106], Loss : 0.6289\nEpoch [12/15], Step [43/106], Loss : 0.6301\nEpoch [12/15], Step [44/106], Loss : 0.6252\nEpoch [12/15], Step [45/106], Loss : 0.6267\nEpoch [12/15], Step [46/106], Loss : 0.6300\nEpoch [12/15], Step [47/106], Loss : 0.6306\nEpoch [12/15], Step [48/106], Loss : 0.6272\nEpoch [12/15], Step [49/106], Loss : 0.6295\nEpoch [12/15], Step [50/106], Loss : 0.6302\nEpoch [12/15], Step [51/106], Loss : 0.6292\nEpoch [12/15], Step [52/106], Loss : 0.6276\nEpoch [12/15], Step [53/106], Loss : 0.6243\nEpoch [12/15], Step [54/106], Loss : 0.6231\nEpoch [12/15], Step [55/106], Loss : 0.6232\nEpoch [12/15], Step [56/106], Loss : 0.6232\nEpoch [12/15], Step [57/106], Loss : 0.6238\nEpoch [12/15], Step [58/106], Loss : 0.6228\nEpoch [12/15], Step [59/106], Loss : 0.6239\nEpoch [12/15], Step [60/106], Loss : 0.6253\nEpoch [12/15], Step [61/106], Loss : 0.6257\nEpoch [12/15], Step [62/106], Loss : 0.6245\nEpoch [12/15], Step [63/106], Loss : 0.6243\nEpoch [12/15], Step [64/106], Loss : 0.6242\nEpoch [12/15], Step [65/106], Loss : 0.6233\nEpoch [12/15], Step [66/106], Loss : 0.6233\nEpoch [12/15], Step [67/106], Loss : 0.6260\nEpoch [12/15], Step [68/106], Loss : 0.6249\nEpoch [12/15], Step [69/106], Loss : 0.6257\nEpoch [12/15], Step [70/106], Loss : 0.6256\nEpoch [12/15], Step [71/106], Loss : 0.6261\nEpoch [12/15], Step [72/106], Loss : 0.6256\nEpoch [12/15], Step [73/106], Loss : 0.6272\nEpoch [12/15], Step [74/106], Loss : 0.6274\nEpoch [12/15], Step [75/106], Loss : 0.6281\nEpoch [12/15], Step [76/106], Loss : 0.6286\nEpoch [12/15], Step [77/106], Loss : 0.6278\nEpoch [12/15], Step [78/106], Loss : 0.6276\nEpoch [12/15], Step [79/106], Loss : 0.6270\nEpoch [12/15], Step [80/106], Loss : 0.6268\nEpoch [12/15], Step [81/106], Loss : 0.6288\nEpoch [12/15], Step [82/106], Loss : 0.6300\nEpoch [12/15], Step [83/106], Loss : 0.6291\nEpoch [12/15], Step [84/106], Loss : 0.6289\nEpoch [12/15], Step [85/106], Loss : 0.6303\nEpoch [12/15], Step [86/106], Loss : 0.6282\nEpoch [12/15], Step [87/106], Loss : 0.6298\nEpoch [12/15], Step [88/106], Loss : 0.6297\nEpoch [12/15], Step [89/106], Loss : 0.6317\nEpoch [12/15], Step [90/106], Loss : 0.6319\nEpoch [12/15], Step [91/106], Loss : 0.6318\nEpoch [12/15], Step [92/106], Loss : 0.6313\nEpoch [12/15], Step [93/106], Loss : 0.6295\nEpoch [12/15], Step [94/106], Loss : 0.6296\nEpoch [12/15], Step [95/106], Loss : 0.6301\nEpoch [12/15], Step [96/106], Loss : 0.6315\nEpoch [12/15], Step [97/106], Loss : 0.6300\nEpoch [12/15], Step [98/106], Loss : 0.6286\nEpoch [12/15], Step [99/106], Loss : 0.6282\nEpoch [12/15], Step [100/106], Loss : 0.6289\nEpoch [12/15], Step [101/106], Loss : 0.6304\nEpoch [12/15], Step [102/106], Loss : 0.6306\nEpoch [12/15], Step [103/106], Loss : 0.6298\nEpoch [12/15], Step [104/106], Loss : 0.6314\nEpoch [12/15], Step [105/106], Loss : 0.6308\nEpoch [12/15], Step [106/106], Loss : 0.6299\nEpoch: 12, Loss: 0.6299, Accuracy: 0.8244\nAccuracy of the network val voices: 66.8929 %\nSaving best model!\nEpoch [13/15], Step [1/106], Loss : 0.4860\nEpoch [13/15], Step [2/106], Loss : 0.4505\nEpoch [13/15], Step [3/106], Loss : 0.4379\nEpoch [13/15], Step [4/106], Loss : 0.4289\nEpoch [13/15], Step [5/106], Loss : 0.4531\nEpoch [13/15], Step [6/106], Loss : 0.4995\nEpoch [13/15], Step [7/106], Loss : 0.4865\nEpoch [13/15], Step [8/106], Loss : 0.5011\nEpoch [13/15], Step [9/106], Loss : 0.4953\nEpoch [13/15], Step [10/106], Loss : 0.5019\nEpoch [13/15], Step [11/106], Loss : 0.4933\nEpoch [13/15], Step [12/106], Loss : 0.4963\nEpoch [13/15], Step [13/106], Loss : 0.5103\nEpoch [13/15], Step [14/106], Loss : 0.5048\nEpoch [13/15], Step [15/106], Loss : 0.5020\nEpoch [13/15], Step [16/106], Loss : 0.5064\nEpoch [13/15], Step [17/106], Loss : 0.5148\nEpoch [13/15], Step [18/106], Loss : 0.5237\nEpoch [13/15], Step [19/106], Loss : 0.5194\nEpoch [13/15], Step [20/106], Loss : 0.5279\nEpoch [13/15], Step [21/106], Loss : 0.5220\nEpoch [13/15], Step [22/106], Loss : 0.5158\nEpoch [13/15], Step [23/106], Loss : 0.5274\nEpoch [13/15], Step [24/106], Loss : 0.5253\nEpoch [13/15], Step [25/106], Loss : 0.5238\nEpoch [13/15], Step [26/106], Loss : 0.5321\nEpoch [13/15], Step [27/106], Loss : 0.5297\nEpoch [13/15], Step [28/106], Loss : 0.5285\nEpoch [13/15], Step [29/106], Loss : 0.5253\nEpoch [13/15], Step [30/106], Loss : 0.5298\nEpoch [13/15], Step [31/106], Loss : 0.5266\nEpoch [13/15], Step [32/106], Loss : 0.5195\nEpoch [13/15], Step [33/106], Loss : 0.5152\nEpoch [13/15], Step [34/106], Loss : 0.5138\nEpoch [13/15], Step [35/106], Loss : 0.5104\nEpoch [13/15], Step [36/106], Loss : 0.5083\nEpoch [13/15], Step [37/106], Loss : 0.5112\nEpoch [13/15], Step [38/106], Loss : 0.5098\nEpoch [13/15], Step [39/106], Loss : 0.5070\nEpoch [13/15], Step [40/106], Loss : 0.5094\nEpoch [13/15], Step [41/106], Loss : 0.5102\nEpoch [13/15], Step [42/106], Loss : 0.5108\nEpoch [13/15], Step [43/106], Loss : 0.5149\nEpoch [13/15], Step [44/106], Loss : 0.5128\nEpoch [13/15], Step [45/106], Loss : 0.5090\nEpoch [13/15], Step [46/106], Loss : 0.5086\nEpoch [13/15], Step [47/106], Loss : 0.5049\nEpoch [13/15], Step [48/106], Loss : 0.5046\nEpoch [13/15], Step [49/106], Loss : 0.5039\nEpoch [13/15], Step [50/106], Loss : 0.5025\nEpoch [13/15], Step [51/106], Loss : 0.5013\nEpoch [13/15], Step [52/106], Loss : 0.5009\nEpoch [13/15], Step [53/106], Loss : 0.5001\nEpoch [13/15], Step [54/106], Loss : 0.4977\nEpoch [13/15], Step [55/106], Loss : 0.4983\nEpoch [13/15], Step [56/106], Loss : 0.4977\nEpoch [13/15], Step [57/106], Loss : 0.4974\nEpoch [13/15], Step [58/106], Loss : 0.4961\nEpoch [13/15], Step [59/106], Loss : 0.4950\nEpoch [13/15], Step [60/106], Loss : 0.4933\nEpoch [13/15], Step [61/106], Loss : 0.4920\nEpoch [13/15], Step [62/106], Loss : 0.4907\nEpoch [13/15], Step [63/106], Loss : 0.4923\nEpoch [13/15], Step [64/106], Loss : 0.4910\nEpoch [13/15], Step [65/106], Loss : 0.4908\nEpoch [13/15], Step [66/106], Loss : 0.4919\nEpoch [13/15], Step [67/106], Loss : 0.4903\nEpoch [13/15], Step [68/106], Loss : 0.4902\nEpoch [13/15], Step [69/106], Loss : 0.4891\nEpoch [13/15], Step [70/106], Loss : 0.4904\nEpoch [13/15], Step [71/106], Loss : 0.4902\nEpoch [13/15], Step [72/106], Loss : 0.4886\nEpoch [13/15], Step [73/106], Loss : 0.4886\nEpoch [13/15], Step [74/106], Loss : 0.4884\nEpoch [13/15], Step [75/106], Loss : 0.4906\nEpoch [13/15], Step [76/106], Loss : 0.4882\nEpoch [13/15], Step [77/106], Loss : 0.4885\nEpoch [13/15], Step [78/106], Loss : 0.4878\nEpoch [13/15], Step [79/106], Loss : 0.4876\nEpoch [13/15], Step [80/106], Loss : 0.4895\nEpoch [13/15], Step [81/106], Loss : 0.4884\nEpoch [13/15], Step [82/106], Loss : 0.4880\nEpoch [13/15], Step [83/106], Loss : 0.4870\nEpoch [13/15], Step [84/106], Loss : 0.4879\nEpoch [13/15], Step [85/106], Loss : 0.4879\nEpoch [13/15], Step [86/106], Loss : 0.4876\nEpoch [13/15], Step [87/106], Loss : 0.4878\nEpoch [13/15], Step [88/106], Loss : 0.4875\nEpoch [13/15], Step [89/106], Loss : 0.4863\nEpoch [13/15], Step [90/106], Loss : 0.4856\nEpoch [13/15], Step [91/106], Loss : 0.4858\nEpoch [13/15], Step [92/106], Loss : 0.4846\nEpoch [13/15], Step [93/106], Loss : 0.4840\nEpoch [13/15], Step [94/106], Loss : 0.4837\nEpoch [13/15], Step [95/106], Loss : 0.4843\nEpoch [13/15], Step [96/106], Loss : 0.4848\nEpoch [13/15], Step [97/106], Loss : 0.4853\nEpoch [13/15], Step [98/106], Loss : 0.4840\nEpoch [13/15], Step [99/106], Loss : 0.4859\nEpoch [13/15], Step [100/106], Loss : 0.4852\nEpoch [13/15], Step [101/106], Loss : 0.4854\nEpoch [13/15], Step [102/106], Loss : 0.4872\nEpoch [13/15], Step [103/106], Loss : 0.4888\nEpoch [13/15], Step [104/106], Loss : 0.4897\nEpoch [13/15], Step [105/106], Loss : 0.4896\nEpoch [13/15], Step [106/106], Loss : 0.4903\nEpoch: 13, Loss: 0.4903, Accuracy: 0.8662\nAccuracy of the network val voices: 67.8961 %\nSaving best model!\nEpoch [14/15], Step [1/106], Loss : 0.2552\nEpoch [14/15], Step [2/106], Loss : 0.3330\nEpoch [14/15], Step [3/106], Loss : 0.3669\nEpoch [14/15], Step [4/106], Loss : 0.3628\nEpoch [14/15], Step [5/106], Loss : 0.3812\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}